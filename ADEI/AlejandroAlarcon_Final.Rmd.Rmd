---
title: "Entrega Final de ADEI"
author: "Alejandro Alarcón"
date: "10/19/2021"
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  html_document:
    toc: no
    toc_depth: '4'
  word_document:
    toc: no
    toc_depth: '4'
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
subtitle: 'Compilación de los entregables 1,2 y 3'
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

\newpage
```{r setup, include=FALSE, results=FALSE}
#Configuración del environment

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
require("knitr")
opts_knit$set(root.dir = "/Users/aleibz/ADEI/ADEI/ADEI/CarPrices/")
```

```{r, echo=FALSE, results=FALSE}
#Limpiamos el environment.

# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())
```


```{r, echo=FALSE, results=FALSE}
#Importamos las librerias y paquetes necesarios.

options(contrasts=c("contr.treatment","contr.treatment"))

requiredPackages <- c("effects","FactoMineR","car", "factoextra","RColorBrewer","ggplot2","dplyr","ggmap","ggthemes","knitr", "missMDA", "epiDisplay", "games")

package.check <- lapply(requiredPackages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})
#verify they are loaded
search()
```

```{r, echo=FALSE, results=FALSE}
#Cargamos los datos

filepath<-"/Users/aleibz/ADEI/ADEI/ADEI/CarPrices/"
load(paste0(filepath,"MyOldCars-Raw.RData"))
```
\newpage
# Introducción al dataset
Echamos un vistazo al dataset
```{r}
summary( df )
names( df )
```

\newpage
## Transformación de variables categóricas a factores
```{r}
#Model
df$model <- factor(paste0(df$manufacturer, "-", df$model))
head(levels(df$model)) #Algunos de los valores para el factor modelo

#Transmission
df$transmission <- factor( df$transmission )
levels( df$transmission )
df$transmission <- factor( df$transmission, levels = c("Manual","Semi-Auto","Automatic"),labels = paste0("f.Trans-",c("Manual","SemiAuto","Automatic")))
head( df )

#FuelType
df$fuelType <- factor(df$fuelType)
levels(df$fuelType)
df$fuelType <- factor( df$fuelType, levels = c("Diesel","Petrol","Hybrid"), labels = paste0("f.Fuel-",c("Diesel","Petrol","Hybrid")))

#Manufacturer
df$manufacturer <- factor(df$manufacturer)
levels(df$manufacturer)
```

\newpage
## Transformación de variables numéricas a factores
```{r}
#Year + Age
summary(df$year)
df$age <-  2021 - df$year 
df$year<-factor(df$year)
summary(df$age)

#EngineSize
summary(df$engineSize)
df$engineSize <- factor(df$engineSize)
table(df$engineSize)
```

## Exploración de las variables

### Factores
```{r}
par(mfrow = c(2, 2))

tab1(df$year)

tab1(df$engineSize)

tab1(df$fuelType)

tab1(df$transmission)
```

### Variables numéricas
```{r}
par(mfrow = c(1,2))
summary(df[c("age", "price", "mileage", "tax", "mpg")])
boxplot( df$age, main="Age" )
hist( df$age )

boxplot( df$price, main="price" )
hist( df$price )

boxplot( df$mileage,main="mileage" )
hist( df$mileage )

boxplot( df$tax, main="tax")
hist(df$tax)

boxplot( df$mpg, main="mpg")
hist(df$mpg)
```

Funciones útiles
```{r}
calcQ <- function(x) {
  s.x <- summary(x)
  iqr<-s.x[5]-s.x[2]
  list(souti=s.x[2]-3*iqr, mouti=s.x[2]-1.5*iqr, min=s.x[1], q1=s.x[2], q2=s.x[3], 
       q3=s.x[5], max=s.x[6], mouts=s.x[5]+1.5*iqr, souts=s.x[5]+3*iqr ) }

countNA <- function(x) {
  mis_x <- NULL
  for (j in 1:ncol(x)) {mis_x[j] <- sum(is.na(x[,j])) }
  mis_x <- as.data.frame(mis_x)
  rownames(mis_x) <- names(x)
  mis_i <- rep(0,nrow(x))
  for (j in 1:ncol(x)) {mis_i <- mis_i + as.numeric(is.na(x[,j])) }
  list(mis_col=mis_x,mis_ind=mis_i) }

countX <- function(x,X) {
  n_x <- NULL
  for (j in 1:ncol(x)) {n_x[j] <- sum(x[,j]==X) }
  n_x <- as.data.frame(n_x)
  rownames(n_x) <- names(x)
  nx_i <- rep(0,nrow(x))
  for (j in 1:ncol(x)) {nx_i <- nx_i + as.numeric(x[,j]==X) }
  list(nx_col=n_x,nx_ind=nx_i) }
```

\newpage
# Por cada variable
## Conteo de missings

Iniciamos el recuento de missings creando dos estructuras de datos auxiliares y llamando a la función countNA. Como podemos apreciar, nos aparecene 13 individuos con valores de missing.
```{r}
imis<-rep(0,nrow(df))  # rows - trips
jmis<-rep(0,2*ncol(df))  # columns - variables

mis1<-countNA(df)
imis<-mis1$mis_ind
inds <- which(imis > 0)
inds
```

Con el siguiente comando podemos ver todos los individuos que hemos seleccionado que contenian algún miss.
En este caso, todos los miss están agrupados en la misma variable: fuelType.
Podríamos pensar que esto podría ser debido a la existencia de coches eléctricos, pero como podemos apreciar, en algunos casos, el tipo de cambio es manual, hecho que no se aplica a los coches eléctricos.
```{r}
df[inds,]
```

Por último, como podemos ver en el recuento de misses por variable, se puede apreciar como todos lo misses se acumulan en la variable que hemos comentado anteriormente: fuelType.
```{r}
mis1$mis_col # Number of missings for the current set of variables
```

En conclusión, solo aparecen un total de 13 missing values en todo el dataframe.
Si miramos por variables, estos 13 missings aparecen en la columna de fuelType.
Por otro lado, y si miramos por individuos, podemos ver como para los 13 individuos que tienen missings, este está en la columna de fuelType.

\newpage
## Conteo de outliers

Iniciamos el recuento de outliers creando dos estructuras de datos auxiliares y una función que retornara los individuos con extreme outliers y mild outliers por separado.
```{r}
iouts<-rep(0,nrow(df))  # rows - trips
jouts<-rep(0,2*ncol(df))  # columns - variables

# Funcion que recibe como parametro una columna y devuelve los ids de los individuos outlier
outliers_column <- function(x){
  outs <- NULL
  out_bounds <- calcQ(x)
  ex <- which((x<out_bounds$souti)|(x>out_bounds$souts))
  mild <- which((x<out_bounds$mouti)|(x>out_bounds$mouts))
  
  boxplot(x)
  abline(h=out_bounds$mouti,col="orange")
  abline(h=out_bounds$mouts,col="orange")
  abline(h=out_bounds$souti,col="red")
  abline(h=out_bounds$souts,col="red")
  
  outs <- rep(0,length(x))
  outs[mild] <- 1
  outs[ex]  <- 2
  if(length(ex)==0){
      outs <- factor(outs, labels=c("Non-Outlier","Mild-Outlier"))
  }else if(length(mild)==0){
      outs <- factor(outs, labels=c("Non-Outlier","Extreme-Outlier"))
  }else{
    outs <- factor(outs, labels=c("Non-Outlier","Mild-Outlier","Extreme-Outlier"))
  }
  list(extreme=ex, mild=mild, outs=outs)
}


```

Aplicaremos la función definida previamente a nuestras columnas numéricas.
```{r}
# Estos son los outliers tanto mild como extreme de las variables numéricas
par(mfrow = c(1, 5))
outs_price <- outliers_column(df$price)
length(outs_price$mild) + length(outs_price$extreme)

outs_mileage <- outliers_column(df$mileage)
length(outs_mileage$mild) + length(outs_mileage$extreme)

outs_tax <- outliers_column(df$tax)
length(outs_tax$mild) + length(outs_tax$extreme)

outs_mpg <- outliers_column(df$mpg)
length(outs_mpg$mild)

outs_age <- outliers_column(df$age)
length(outs_age$mild) + length(outs_age$extreme)
```

Generamos la suma de todos los outliers.
```{r}
df$outs <- rep(0,nrow(df))
df$outs[which(outs_mileage$outs!="Non-Outlier")] <- df$outs[which(outs_mileage$outs!="Non-Outlier")] + 1
df$outs[which(outs_price$outs!="Non-Outlier")] <- df$outs[which(outs_price$outs!="Non-Outlier")] + 1
df$outs[which(outs_tax$outs!="Non-Outlier")] <- df$outs[which(outs_tax$outs!="Non-Outlier")] + 1
df$outs[which(outs_mpg$outs!="Non-Outlier")] <- df$outs[which(outs_mpg$outs!="Non-Outlier")] + 1
df$outs[which(outs_age$outs!="Non-Outlier")] <- df$outs[which(outs_age$outs!="Non-Outlier")] + 1

summary(df$outs)
```

```{r}
# Conteo de extreme outliers
sum_extreme_outliers <- length(outs_price$extreme) + length(outs_mileage$extreme) + length(outs_tax$extreme) + length(outs_mpg$extreme) + length(outs_age$extreme)
sum_extreme_outliers
# Conteo de mild outliers
sum_mild_outliers <- length(outs_price$mild) + length(outs_mileage$mild) + length(outs_tax$mild) + length(outs_mpg$mild) + length(outs_age$mild)
sum_mild_outliers
```
Como podemos apreciar, una vez realizado el sumatorio de los diferentes tipos de outliers para todo el dataframe, vemos que el número de extreme outliers es mayor al de mild outliers.

Ponemos los extreme outliers como missings.
```{r}
df[outs_mileage$extreme,"mileage"]<-NA
df[outs_tax$extreme,"tax"]<-NA
df[outs_mpg$extreme,"mpg"]<-NA
df[outs_age$extreme,"age"]<-NA
```
\newpage
## Conteo de errores

Procedemos al conteo de los errores.

Para este apartado, en la mayoría de los casos, simplemente comprobamos que los valores insertados no sean negativos o cero, aunque tal vez se podrían tener en cuenta otros casos si se examina el dataframe de manera técnica (i.e. consumos o distancias exageradamente elevados).
```{r}
# Price
  err_price <- which(df$price<0)
  #df <- df[ -err_price, ]    #En este caso está vacío, así que no es necesario   
  length(err_price)
# Age
  err_age <- which(df$age<0)
  df[err_age,"age"] <- NA
  length(err_age)
# Mileage
  err_mileage <- which(df$mileage<0)
  df[err_mileage,"mileage"] <- NA
  length(err_mileage)
# mpg
  err_mpg <- which(df$mpg<0)
  df[err_mpg,"mpg"] <- NA
  length(err_mpg)
# engineSize
  err_engineSize <- which(df$engineSize == "0")
  df[err_engineSize,"engineSize"] <- NA
  length(err_engineSize)
# tax
  err_tax <- which(df$tax<0)
  df[err_tax,"tax"] <- NA
  length(err_tax)
```

\newpage
# Imputación

A continuación, vamos a proceder a la imputación de los missings, errors y extreme outliers que hemos encontrado previamente.

En primer lugar, separaremos nuestras variables en numéricas y categóricas. También declaramos price y manufacturer como variables respuesta.
```{r}
library(missMDA)
names(df)
vars_num <- names(df)[c(5,7,8,11)]
vars_cat <- names(df)[c(1,2,4,6,9)]
vars_res <- names(df)[c(3,10)]
```
\newpage
## Imputación de variables numéricas

Podemos echar un pequeño vistazo a nuestras variables numéricas.
```{r}
summary(df[,vars_num])
```

Aplicamos la función imputePCA con 2 componentes primarias.
(Teniendo en cuenta el PCA que realizamos en laboratorios posteriores, dos componentes son suficientes para aglutinar más del 80% de la variabilidad).
```{r}
res.impca <- imputePCA(df[,vars_num], ncp = 2)
summary(res.impca$completeObs)
```

Vamos a ver para cada variable, el valor que acaban recibiendo los individuos que hemos inputado:
```{r}
head(res.impca$completeObs[ outs_age$extreme, "age" ])
head(res.impca$completeObs[ outs_mileage$extreme, "mileage" ])
head(res.impca$completeObs[ outs_tax$extreme, "tax" ])
head(res.impca$completeObs[ outs_mpg$extreme, "mpg" ])
```

Sustituimos en el dataframe original:
```{r}
df[, vars_num] <- res.impca$completeObs
```

Como podemos ver, en los valores que previamente teniamos NA, ahora aparecen los valores que se han obtenido de la imputación.
```{r}
df[outs_age$extreme,"age"]
df[outs_mpg$extreme,"mpg"]
```
\newpage
## Imputación de factores

Vamos a empezar echando un vistazo al summary de las variables categóricas. 

Podemos apreciar que en el caso de fuelType, nos aparecen 13 valores NA (los missings que hemos encontrado). 

En el caso de engineSize, podemos ver que aparecen 9 NAs (los errores que hemos encontrado).
```{r}
summary(df[,vars_cat])
```

Procedemos con la imputación:
```{r}
res.immca <- imputeMCA(df[,vars_cat],ncp=10)
summary(res.immca$completeObs)
```
Podemos ver como en el summary que acabamos de realizar, ya no aparecen valores NA para ninguna de las variables.

Sustituimos la salida de la imputación en nuestro dataframe:
```{r}
df[,vars_cat]<-res.immca$completeObs
```

\newpage
# Discretización de variables numéricas en factores

Una vez hemos realizado la imputación, tanto de las variables numéricas como de los factores, vamos a proceder a la discretización de las variables numéricas.

En los casos de age o tax, esta discretización se ha hecho de manera pseudo-aleatoria, ya que la discretización por cuantiles no funcionaba bien debido a la distribución de estas variables.
```{r}
vars_num
```

Como hemos podido ver con anterioridad, las distribuciones de estas variables son bastante diferentes entre sí.

En el caso de price o mpg, por ejemplo, podemos ver que la distribución se aproxima más a una normal.

Por otro lado, en la variable mileage, esta se podría aproximar mejor con una distribuxión de chi-cuadrado o exponencial.

Finalmente, para la variable tax, podemos ver que existe una gran acumulación en valores cercanos a 145.

A continuación lo estudiaremos con más profundidad.

\newpage
## Mileage
En primer lugar vamos a echar un vistazo a esta variable.
```{r}
summary(df$mileage)
par(mfrow = c(2,1))
boxplot(df$mileage,horizontal = TRUE)
hist(df$mileage, breaks=100)
```

Procedemos a la discretización según cuantiles, ya que en este caso si que era bastante representativa.
```{r}
quants <- quantile(df$mileage,seq(0,1,0.25),na.rm=TRUE)

df$aux <- factor(cut(df$mileage, breaks=quants[1:5], include.lowest = TRUE))
df$f.miles<-factor(cut(df$mileage/1000,breaks=quants[1:5]/1000,include.lowest = TRUE ))
levels(df$f.miles)<-paste("f.miles-",levels(df$f.miles),sep="")
table(df$f.miles)
#Esto cuadra ya que estamos tomando como referencia los quantiles y podemos ver como en todos hay el mismo número de elementos
```
Como podemos apreciar, aparecen 1250 elementos en cada cuantil, que es lo que esperariamos en condiciones ideales.

\newpage
## Tax
Para la variables tax la cosa se complica un poco.

Si echamos un vistazo a algunos gráficos, podemos ver como hay una gran acumulación en valores entre 140 y 155. Esto hace que, al generar los cuantiles, el q1 y el q2 tengan el mismo valor, de modo que vamos a proceder con una discretización alternativa.
```{r}
summary(df$tax)
par(mfrow = c(2, 1))
boxplot(df$tax, horizontal = TRUE)
hist(df$tax, breaks=200)


df$aux <- factor(cut(df$tax, breaks=c(0,144,145,155,205), include.lowest = TRUE))
table(df$aux)
```
En la tabla podemos ver el gran pico en el valor 145, que acumula 2647 elementos, más de la mitad del tamaño de la muestra.


Por último, generamos una variable factor con etiquetas para los diferentes intervalos que hemos definido.
```{r}
df$f.tax<-factor(cut(df$tax,breaks=c(0,144,145,155,205),include.lowest = TRUE ))
levels(df$f.tax)<-paste("f.tax-",levels(df$f.tax),sep="")
table(df$f.tax)
```

\newpage
## Mpg
En el caso de la variable mpg, vamos a proceder con una discretización por cuantiles, ya que su distribución lo permite y el resultado es bastante explicativo junto con las etiquetas que vamos añadir al factor.
```{r}
summary(df$mpg)
par(mfrow = c(2, 1))
boxplot(df$mpg, horizontal = TRUE)
hist(df$mpg)

quants <- quantile(df$mpg,seq(0,1,0.25),na.rm=TRUE)

df$aux <- factor(cut(df$mpg, breaks=quants[1:5], include.lowest = TRUE))
df$f.mpg<-factor(cut(df$mpg,breaks=quants[1:5],include.lowest = TRUE ))
levels(df$f.mpg)<-paste("f.mpg-",c("muy bajo","bajo","medio","alto"),sep="")
table(df$f.mpg)
```

\newpage
### Age

La variable age, similarmente a la variable mileage, tiene una distribución que se acumula en valores bajos. De modo que vamos a aplicar una dicretización según los primeros cuantiles generando tres intervalos.
```{r}
summary(df$age)
par(mfrow = c(2, 1))
boxplot(df$age,horizontal = TRUE)
hist(df$age)

quants <- quantile(df$age,seq(0,1,0.25),na.rm=TRUE)

df$aux <- factor(cut(df$age, breaks=c(quants[1:3],max(df$age)), include.lowest = TRUE))
summary(df$aux)
df$f.age<-factor(cut(df$age,breaks=c(quants[1:3],max(df$age)),include.lowest = TRUE ))
levels(df$f.age)<-paste("f.age-",c(levels(df$f.age)[1:2],"(+4)"),sep="")
table(df$f.age)
```



\newpage
### Price

Por último, vamos a discretizar nuestro target numérico price. En este caso, lo vamos a dividir en 8 categorias ya que es uno de los requisitos que se marcan para entregas posteriores.
```{r}
quants <- quantile(df$price,seq(0,1,0.125),na.rm=TRUE)

df$aux <- factor(cut(df$price, breaks=c(quants[1:8],max(df$price)), include.lowest = TRUE))
summary(df$aux)
df$f.price<-factor(cut(df$price,breaks=c(quants[1:8],max(df$price)),include.lowest = TRUE ))
levels(df$f.price)<-paste("f.price-",c(levels(df$f.price)),sep="")
table(df$f.price)
```

```{r,include=FALSE}
df["aux"] <- NULL
```

\newpage
## Generación del target categórico Audi
Vamos a proceder a generar nuestro target categórico.

Como indica la documentación de la práctica, este va a consistir en una variable que nos indique si el fabricante de un vehículo es Audi.
```{r}
df$Audi<-ifelse(df$manufacturer == "Audi",1,0)
df$Audi<-factor(df$Audi,labels=paste("Audi",c("No","Yes")))
summary(df$Audi)
```

A continuación algunos plots que nos ayudan a explicar esta varible.
```{r}
par(mfrow = c(1, 2))
# Pie
piepercent<-round(100*(table(df$Audi)/nrow(df)),dig=2); piepercent
pie(table(df$Audi),col=heat.colors(2),labels=paste(piepercent,"%"))
legend("topright", levels(df$Audi), cex = 0.8, fill = heat.colors(2))
# Bar Chart
barplot(table(df$Audi),col=c("red","yellow"))
```

\newpage
# Identificación los outliers multivariantes

En este apartado vamos a proceder con la identificación de los outliers multivariante.
En primer lugar, vamos a cargar la librería y echaremos un primer vistazo a los posibles outliers.
```{r}
library(mvoutlier)

ll<-which(is.na(df$price)) #vacío
summary(df[,c(vars_res[1],vars_num)])
```

La ejecución de la función aq.plot (Adjusted Quantile) genera 4 gráficos. 
  * En el de arriba a la izquierda podemos ver los datos originales.
  * En el de arriba a la derecha podemos ver la aproximación de estos datos a una distribucion de chi-cuadrado.
  * En el de abajo a la izquierda podemos ver los outliers determinados por el cuantil especificado de la chi-cuadrado (99.5%).
  * En el de abajo a la derecha podemos ver los outliers determinados por el Adjusted Quantile (99.5%).
```{r}
mout<-aq.plot(df[,c(vars_res[1],vars_num)],delta=qchisq(0.995,5),quan=0.995)
```

A continuación, vamos a usar la función Moutlier para mostrar más información sobre los posibles outliers segun la distancia de Mahalanobis. Cabe destacar que el output de esta función nos devuelve tanto la distancia de Mahalanobis clásica como la robusta.

Esta función se debe aplicar a conjuntos de datos qus siguen una distribución normal, de modo que, según el estudio que hemos realizado previamente, solo lo aplicaremos a las variables de price, mileage, mpg y distance.

Disclaimer: Para las variables que menciono anteriormente, el programa funciona bien. Si añadimos tax, falla. Pero si nos fijamos en los histogramas/barplots de las variables que mencionamos, las únicas en las que podríamos suponer una normalidad en los datos serían price y mpg, ya que age o mileage tienden a una acumulación en valores bajos siguiendo distrubuciones que podrían ser parecidas a chi-cuadrado, log normal o exponencial.

Seguimos con el análisis de Outliers Multivariantes.

En este caso, aplicaremos la función Moutlier a las variables que hemos mencionado anteriormente, mostrando las distancias de Mahalanobis clásica y robusta. Además, esta función nos genera un cutoff a partir del cual podemos detectar los outliers.
```{r}
library(chemometrics)
mout<-Moutlier(df[,c("price","mileage","mpg","age")],quantile = 0.995, plot = TRUE)
ll<-which(mout$md > mout$cutoff)
```

Vamos a echar un vistazo a las propiedades de las individuos considerados como Multivariant Outliers.
```{r}
summary(df[ll,c("price","mileage","mpg","age")])
```

Finalmente, crearemos una variable auxiliar que nos marcará, para cada individuo, si es Multivariant Outlier o no.
```{r}
df$mout <- 0
df$mout[ ll ]<-1
df$mout <- factor( df$mout, labels=c( "NoMOut","YesMOut"))
table(df$mout)
```

\newpage
# Profiling

Por último, vamos a realizar el profiling de nuestro dataframe según nuestras variables target: Audi como variable categórica y price como variable numérica.

## Target numérico (Price)
```{r}
library(FactoMineR)
summary(df$price)
vars_num <- c("mileage","tax","mpg","age")
vars_cat <- c("model","year","transmission","fuelType","engineSize","f.miles","f.tax","f.mpg","f.age","Audi")
```

Vamos a proceder al profiling del target numérico a partir de la función condes del paquete FactoMineR
```{r}
res.condes<-condes(df[,c("price",vars_num,vars_cat,"manufacturer")],1)
```

```{r}
res.condes$quanti  # Global association to numeric variables
```
Como podemos apreciar en la anterior salida, las variables numéricas que más correlación tienen con nuestra variable target (price) son mileage, mpg y age. En este caso, cabe destacar que se correlacionan de manera negativa. Esto indica que son inversamente proporcionales, es decir, que a más mileage/mpg/age, menor price (y viceversa).

```{r}
res.condes$quali # Global association to factors
```
En el caso de las variables categóricas (o factores), podemos apreciar que existe una clara relación entre los factores model, year, engineSize. También con algunos de los factores que hemos creado derivados de las variables numéricas. Vamos a analizarlo más a fondo:

Ahora nos fijaremos en las primeras lineas de la salida de condes$category:
```{r}
head(res.condes$category)  # Partial association to significative levels in factors
```
En esta salida podemos apreciar como para coches nuevos, y con un consumo o kilometraje bajos, el precio estimado es más alto.
Como casos notables también podemos mencionar algunos casos de coches premium, para los que el valor estimado es más alto:
```
  model=Mercedes- G Class         45588.6851  5.645865e-07
  model=BMW- X7                   42925.9709  9.727955e-37
```
Y si nos fijamos en las últimas:
```{r}
tail(res.condes$category)
```
Podemos apreciar como factores como el cambio de marchas manual, un kilometraje alto o un cosumo alto tienden a abaratar el vehículo.

Algunos casos especiales son modelos de VW, que ven su precio realmente reducido:
```
  model=VW- CC                   -17098.5649  7.630716e-03
  model=VW- Beetle               -20721.5371  1.122196e-04
```  

## Target factor (AUDI)
Vamos a proceder a ejecutar la función catdes con para identificar las asociaciones hacia el target categórico que hemos generado.
Para esto, y ya que simplemente considero que no es indicativo, no usaré las variable modelo ni manufacturer, ya que no aportan ningún tipo de infomación al análisis.
```{r}
res.catdes<-catdes(df[,c("Audi",vars_num,vars_cat[2:9])],1)
```

Procedemos a ver las variables que parecen estar más correlacionadas con nuestra variable target:
```{r}
res.catdes$quanti.var  # Global association to numeric variables
```

Si lo analizamos un poco más las relaciones con variables numéricas, podemos ver que los vehículos Audi tienen consumos más bajos y kilometrages, antigüedad e impuestos más altos que la muestra que estudiamos. Si analizamos los vehículos que no son Audi, veremos lo contrario.
```{r}
res.catdes$quanti # Partial association of numeric variables to levels of outcome factor
```

Si estudiamos las varibles categóricas, podemos ver que los factores generados con las discretizaciones que hemos realizado anteriormente se relacionan de manera estrecha con nuestro target, además del engineSize, el fuelType o la transmission.
```{r}
res.catdes$test.chi2 # Global association to factors
```

Por último, vamos a entrar más en detalle en la relación de las variables categóricas con nuestro target.
```{r}
res.catdes$category  # Partial association to significative levels in factors
```
Vamos a interpretar un poco la salida anterior.
Si cogemos este ejemplo,
```
`Audi No`                     
                                Cla/Mod     Mod/Cla Global      p.value     v.test
engineSize=2.1                100.00000 10.47280122   8.24 9.327968e-46  14.198736
```
podemos entender como el 100% de los vehiculos con engineSize=2.1, no están fabricados por Audi. También podemos extraer que el 10.47% de los vehículos no fabricados por Audi tienen engineSize=2.1.

Otro ejemplo:
```
`Audi Yes`
                                 Cla/Mod    Mod/Cla Global      p.value     v.test
transmission=f.Trans-Manual    24.342473 40.8067542  35.74 1.111890e-04   3.864781
```
En este caso podemos ver como el 24.34% de los vehículos con transmisión manual son Audi. Además, el 40.80% de los vehículos fabricados por Audi, tienen transmisión manual.


\newpage
# Análisis de Componentes Principales

En primer lugar, vamos a echar un vistazo a nuestras variables.
Las vamos a mostrar en un orden concreto que nos va ayudar más tarde a referirnos a ellas a partir de sus índices.
```{r, echo=FALSE, results=FALSE, include=FALSE}
vars_res<-c("price","Audi")
vars_cat<-c("model", "year", "transmission", "fuelType", "engineSize", "manufacturer", "f.price", "f.miles", "f.mpg", "f.tax", "f.age")
vars_num<-c("mileage", "tax", "mpg", "age")

c(vars_res, vars_cat,vars_num, "mout")
summary( df[ , c(vars_res, vars_cat, vars_num, "mout") ] )
```

Realizamos el análisis PCA de nuestro dataset, pasando como variables suplementarias qualitativas todos nuestros factores, y como variable suplementaria cuantitativa nuestro target numérico price. Además, añadimos los individuos que hemos categorizado como multivariant outliers como individuos suplementarios, para que no introduzcan ruido a la hora de calcular el PCA.
```{r}
ll <- which( df$mout == "YesMOut")
res.pca<-PCA(df[,c(vars_res, vars_cat, vars_num)],quali.sup=c(2:13),quanti.sup= c(1), ind.sup = ll )
```

Como podemos apreciar en los gráficos previos, las dos primeras componentes principales aglutinan un 52,64% y un 30,09% de la variabilidad respectivamente.

Podemos apreciar también como la primera componente principal parece aglutinar la variabilidad de las variables mileage y age, que se proyectan en la misma dirección, solapandose en el diagrama, mientras que la segunda componente principal aglutina sobretodo la variabilidad de la variable tax. Asímismo, parece que las dos componentes reflejan la variable mpg por igual, sin embargo en el caso de la Dim2 esta relación es negativa.

Además de esto, podemos ver la proyección de nuestro target numérico price en el plano formado por las dos primeras componentes principales. Se puede apreciar que price está más relacionada con la primera componente que con la segunda, pero de manera negativa.

Por último, podemos ver la proyección de los individuos en el plano formado por las dos componentes principales. Cabe destacar un pequeño grupo de individuos en la esquina inferior izquierda del plot.
```{r}
plot.PCA(res.pca,choix=c("ind"),invisible=c("ind"))
```

```{r}
res.pca$eig
```
Se puede ver que se han creado 4 componentes diferentes, cuyos Eigenvalues normalizados son 2,106, 1,203, 0,510 y 0,181 respectivamente. Si seguimos el criterio de Kaiser, como estos eigenvalues ya se han normalizado, deberíamos quedarnos con aquellos componentes con eigenvalues superiores a 1, de modo que nos quedaríamos con las dos primeras componentes.

En la siguiente salida, podemos ver que, como ya habíamos mencionado previamente, la variable tax tiene más correlación con el segundo componente que en el primero, mientras que podemos ver que pasa lo contrario con las variables mileage o age. También podemos ver que la variable price se relaciona más con la primera componente aunque lo hace de manera negativa.
```{r}
res.pca$var$cor
```

En el siguiente plot podemos ver los eigenvalues de las cuatro componentes que se han generado.
```{r}
barplot(res.pca$eig[,1],main="Eigenvalues",names.arg=paste("dim",1:nrow(res.pca$eig)))
```

## Análisis según los individuos
A continuación, vamos a proceder a analizar los individuos.

En la salida a continuación, podemos apreciar:
En las dos primeras columnas, las coordenadas que reciben los individuos para las dos primeras componentes principales.
En las siguientes dos columnas, los valores correspondientes al cos2 para las dos dimensiones correspondientes a cada individuo.
En las dos últimas columnas, la contribución que tienen los individuos en cada componente principal.
```{r}
head(round(cbind(res.pca$ind$coord[,1:2],res.pca$ind$cos2[,1:2],res.pca$ind$contrib[,1:2]),2))
```

En la siguiente salida, podemos apreciar, de manera ordenada para la primera dimensión, los individuos que más contribución tienen hacia la primera dimensión y así como sus contribuciones para el resto de dimensiones.
```{r}
inds <- res.pca$ind$coord
inds <- as.data.frame(inds)
rang.dim1<-inds[order(inds$Dim.1, decreasing = TRUE),]
head(rang.dim1)
```

Vamos a proceder a hacer lo mismo para la segunda dimensión.
```{r}
rang.dim2<-inds[order(inds$Dim.2, decreasing = TRUE),]
head(rang.dim2)
```
Como se puede apreciar en las salidas anteriores, podemos ver como parece haber elementos con más contribución en la segunda dimensión que en la primera. 

A continuación, podemos ver todas las variables de los 10 primeros individuos que más aportan a la primera componente principal:
```{r}
df[which(row.names(df) %in% row.names(res.pca$ind$coord[row.names(rang.dim1)[1:10],])),]
```
Como se puede apreciar, la mayoría tienen valores altos para las variables age y mileage, que son las que más se relacionan con la primera componente. Contrariamente, por norma general, estos vehículos también tienen precios menores, cosa que era esperable si tenemos en cuenta que la proyección de la variable price en el plano formado por las dos componentes principales tiene sentido negativo.

## Análisis según las variables

### Variables numéricas
Desde el punto de vista de las variables, en la salida a continuación, podemos ver los valores correspondientes al cos2 para cada una de las componentes principales en las dos primeras columnas, así como las contribuciones que tienen hacia estas en las dos últimas. Cabe destacar las contribuciones de las variables age y mileago para la primera dimensión.
```{r}
round(cbind(res.pca$var$cos2[,1:2],res.pca$var$contrib[,1:2]),2)
```

A continuación podemos ver las correlaciones de todas nuestras variables numéricas con la primera componente principal. Cabe destacar las fuertes correlaciones con age y mileage, como habíamos mencionado previamente.
```{r}
res.des<-dimdesc(res.pca)
res.des$Dim.1$quanti
```

Por último, y aunque hemos afirmado que, según el criterio de Kaiser, solo eran necesarias 2 componentes principales, vamos a analizar graficamente las componentes 3 y 4.
Podemos apreciar como las variables tax y mpg tienen gran importancia en la tercera componente, mientras que para la cuarta tienen más importancia mileage y age.

```{r}
plot.PCA(res.pca,choix=c("var"),axes=c(3,4))
```

En el siguiente gráfico, podemos ver gráficamente y de manera resumida la importancia que tienen cada una de las variables numéricas en las diferentes componentes principales.
```{r}
library("corrplot")
corrplot(res.pca$var$cos2, is.corr=FALSE)
```

### Targets
En el siguiente plot, podemos apreciar como nuestro target categórico AUDI, tiene más relación con la segunda dimensión que con la primera.
```{r}
ll <- which( df$mout == "YesMOut")
res.pca<-PCA(df[,c(vars_res, vars_num)],quali.sup=c(2),quanti.sup= c(1), ind.sup = ll, ncp=2)
plot(res.pca, choix="ind",invisible=c("ind","ind.sup"), cex=0.7, graph.type = "classic")
```

Podemos ver como la componente más representativa de este target es la segunda, ya que las coordenadas que aparecen vienen dadas por la correlación.
```{r}
res.pca$quali.sup$coord 
```

### Factores
A continuación podemos ver los coeficientes R-squared que aparecen para nuestros factores:
Cabe destacar la variabilidad de la componente que viene explicada por la variabilidad de factores como year o f.miles. Si lo analizamos, nos daremos cuenta de que estos factores se crearon en la entrega anterior a partir de las variables mileage y age, de modo que es coherente que los estos factores derivados también tengan una alta explicabilidad de la varianza de la primera componente.
```{r}
res.des$Dim.1$quali
```

Para la segunda componente, podemos ver como los factores más relevantes son f.tax y f.mpg.
```{r}
res.des$Dim.2$quali
```

En resumen, hemos podido ver que tras realizar el PCA, las dos primeras componentes son capaces que aglutinar un 82.73% de la variablidad de nuestro target. Las dos variables que más peso tienen para la primera componente son mileage y age, mientras que para la segunda, la variable más relevantes es tax.

También podemos ver como al añadir los factores como suplementarios, los factores derivados de las variables numéricas adquiren una alta correlación con aquellas componentes donde sus variables numéricas asociadas tienen un peso mayor.

\newpage
# Hierachical Clustering
A continuación, y para ser prácticos, vamos a proceder a realizar el proceso de clustering jerárquico, a partir del cual vamos a determinar (o al menos aproximar) el número óptimo de clusters para ejecutar el clustering con K-means.

En primer lugar, vamos a volver a ejecutar el PCA con las variables categoricas como suplementarias y quedándonos solo con las dos primeras componentes principales, como se ha determinado anteriormente con el criterio de Kaiser para el ACP.
```{r}
res.pca<-PCA(df[,c(vars_res, vars_cat, vars_num)],quali.sup=c(2:13),quanti.sup= c(1), ind.sup = ll, ncp = 2)
```

A continuación ejecutaremos el HCPC a partir del ACP anterior. Con el parámetro nb.clust=-1 indicamos al sistema que tome el número óptimo de clusters según la partición con la que el decremento relativo de inercia es más alto. (Según la documentación - (i(clusters n+1)/i(cluster n)) ). Podemos ver que para este caso, se ha seleccionado como óptima una partición de 3 clusters.
```{r}
res.hcpc<-HCPC(res.pca,order=TRUE, nb.clust=-1)
```

Si aplicamos el criterio de Kaiser, podemos ver que dos componentes son sufientes.
```{r}
length(which(res.hcpc$call$t$res$eig[,1] > mean(res.hcpc$call$t$res$eig[,1])))
```

Según los dos criterios aplicados (el que aplica el sistema con el parámetro nb.clust=-1 y el de Kaiser) obtenemos que el número óptimo de componentes es 2 o 3. Procederemos con 3 componentes ya que así aglomeramos mayor variabilidad.

Ejecutando el siguiente comando, podemos ver como se relacionan las dos componentes principales a partir de las cuales hemos generado la clusterización, con los diferentes clusters que se han generado.
Se puede apreciar como para el cluster 1, la coordenada de la primera componente principal es significativamente más baja que para el conjunto del datafrfame.
En el caso del cluster 2, ambas componentes principales tienen un valor mayor.
Por último, para el cluster 3, la primera componente tiene un valor mayor mientras que la segunda tiene un valor menor.
```{r}
res.hcpc$desc.axes
```

## Análisis según las variables
A continuación vamos a ver como se relacionan las variables originales del dataframe con los clusters que se han generado.

En primer lugar, podemos ver el número de individuos que se han asignado a cada cluster.
```{r}
summary(res.hcpc$data.clust$clust)
```

### Factores
A partir del test de chi2, se puede determinar que factores diferencian los clusters que se han generado.
```{r}
res.hcpc$desc.var$test.chi2
```

Si profundizamos un poco mas en esto, podemos ver como caracterizan los valores de las variables cualitativas los distintos clusters que se han generado.

<He omitido la salida porque ocupa mucho, pero abajo podemos ver algunas de las conlcusiones>

```{r}
#res.hcpc$desc.var$category
```
Podemos destacar, por ejemplo la acumulación de coches nuevos y con poco kilometraje en el primer cluster(f.age=f.age-[1,2] -> Mod/Cla 76.0016694)

### Variables numéricas
Con el test eta-squared, podemos determinar qué variables numéricas han sido influyentes a la hora de generar la clusterización.
```{r}
res.hcpc$desc.var$quanti.var
```
Podemos ver que las variables más representativas son age, mileage y tax, que son las mismas que aparecen como más determinantes a la hora de realizar el PCA.

Por último, podemos analizar como estas variables numéricas caracterizan los distintos clusters.
```{r}
res.hcpc$desc.var$quanti
```
En el cluster 1, podemos apreciar como aparecen coches con precios más altos y menores kilometrajes y años, de modo que este cluster está formado por coches nuevos y con poco uso, que tienen asociado un precio más elevado.

En el cluster 2, podemos ver como aparecen coches ás viejos y con más kilometraje, con un precio más bajo. 

Por último, podríamos calificar el cluster 3 como el cluster 'ECO', ya que aparecen coches con un consumo muy bajo y que han pagado menos impuestos, puede que debido a subvenciones.

## Análisis según los inividuos

Vamos a analizar los paragons de cada cluster que se ha derivado a partir del clústering jerárquico.
```{r}
res.hcpc$desc.ind$para
```

Si analizamos los paragons del primer cluster, podemos ver como aparecen coches nuevos de gasolina. Todos tienen kilometrajes e impuestos similares.
```{r}
summary(df[c("30871","24055","28264","48543","34122"),])
```

Por último, echaremos un vistazo a los individuos más típicos de los clusters:
```{r}
res.hcpc$desc.ind$dist
```

Si analizamos el primer cluster, podemos ver que todos son del 2020 y en su mayoría semi-áutomáticos. Además, tienen kilometrajes y consumos muy bajos. Todos están en el rango más alto de precio.
```{r}
summary(df[c("31816","17634","17208","48339","24771"),])
```

Podemos ver amplias diferencias entre los individuos típicos de los clusters y los paragons.

\newpage

# K-means Clustering desde ACP
```{r}
res.pca<-PCA(df[,c(vars_res, vars_cat, vars_num)],quali.sup=c(2:13),quanti.sup= c(1), ind.sup = ll, ncp=2)
ppcc <- res.pca$ind$coord
kc <- kmeans (dist(ppcc),3)
df[-ll, "claKMPCA"] <- kc$cluster
```

Podemos ver como dentro de nuestra variable kc, se guardan datos como el cluster al que se asigna cada elemento, las distancias en el interior de los clusters o las distancias entre clusters. 
```{r}
summary(kc)
```

Estos valores nos ayudan a determinar la calidad de la cluserización, ya que idealmente, queremos clusters con elementos muy juntos entre si y mucha diferenciación entre clusters.
Para var la calidad de la clusterización, vamos a realizar el cociente de las distancias entre clusters entre la suma de todas las distancias.
```{r}
kc$betweenss/kc$totss
```
Podemos ver que la suma de las distancias entre los clusters suma un 70% del total.

Por otro lado, si comprobamos la suma de las distancias dentro de los clusters, podemos ver como estas tan solo suman el 30% del total.
```{r}
kc$tot.withinss/kc$totss
```

De este modo, podemos determinar que la calidad de la clusterización es relativamente buena ya que la distancia entre clusters es grande (los clusters están diferenciados entre si), pero las distancias dentro de los clusters son pequeñas (los clusters están formados por elementos muy parecidos).

A continuación, vamos a mostrar un plot donde se muestran claramente los clusters de diferentes colores. 
```{r}
fviz_cluster(kc, data=ppcc)
```
En este gráfico podemos ver como no esxiste una definición tan clara como esperábamos en la clusterización.

Por último, vamos a volver a mostrar el gráfico resultande del clustering jerárquico, para poder comparar mejor los resultados:
```{r}
plot.HCPC(res.hcpc, choice="map")
```
Podemos ver como los dos procesos de clusterización han llevado a resultados realmente diferentes. A primera vista, se puede ver como la clusterización jerárquica da un resultado mucho más comprensible en el plano de las dos componentes que se han usado para realizar la clusterización.

Por otro lado, en los dos procesos se ha definido el primer cluster de manera muy similar.

Vamos a analizar los clusters que se han creado a partir de K-Means.
```{r}
df$claKMPCA <- factor(df$claKMPCA)
km.catdes<-catdes(df[,c("claKMPCA",vars_num,vars_cat, "price")],1)
km.catdes$quanti
```
Podemos ver como el primer cluster esta formado por coches con consumo bajo y con impuestos bajos, lo que sería equivalente al cluster 3 'ECO' que hemos obtenido en la clusterización jerárquica.

Para el cluster 2, podemos ver como aparecen coches más baratos pero viejos y con más kilometraje.

En el cluster 3, aparecen los coches más caros, nuevos y con menor kilometraje. Curioso que también tengan mayor consumo.

\newpage
# Correspondence Analysis
Vamos a proceder a realizar el análisis de correspondencias entre variables.

En primer lugar, vamos a analizar la correspondencia entre f.price y f.miles.
```{r}
tt<-table(df[,c("f.price","f.miles")])
res.ca<-CA(tt)
chisq.test(tt)
```

Podemos ver que la mayoría de la variabilidad se acumula en la primera componente (87%).
```{r}
fviz_eig(res.ca)
```

Si realizamos los plots, podemos ver la presencia del efecto Guttman que nos indica que las variables estan fuertemente relacionadas.
```{r}
plot( res.ca, cex=0.8, graph.type = "classic" )
lines( res.ca$row$coord[,1], res.ca$row$coord[,2], col="blue", lwd = 2 )
lines( res.ca$col$coord[,1], res.ca$col$coord[,2], col="red", lwd = 2 )
```

En el siguiente gráfico se puede ver como los coches con menor kilometraje tienen precios más altos y viceversa. 
```{r}
fviz_ca_biplot(res.ca,repel=TRUE)+theme_bw() 
```

A continuación, vamos a realizar el mismo proceso para las variables f.price y f.age.
```{r}
tt<-table(df[,c("f.price","f.age")])
res.ca<-CA(tt)
```

En este caso, también se puede ver la clara relación que hay entre las variables, donde los coches más nuevos son más caros y los viejos más baratos.
```{r}
plot( res.ca, cex=0.8, graph.type = "classic" )
lines( res.ca$row$coord[,1], res.ca$row$coord[,2], col="blue", lwd = 2 )
lines( res.ca$col$coord[,1], res.ca$col$coord[,2], col="red", lwd = 2 )
```

Por último, vamos a analizar la relación entre f.price y manufacturer.
En este caso, se puede ver claramente los coches VW tienen precios más baratos que los BMW, Audi o Mercedes.
```{r}
tt<-table(df[,c("f.price","manufacturer")])
res.ca<-CA(tt)
```

En este caso, en el siguiente gráfico se puede ver claramente los coches VW tienen precios más baratos que los BMW, Audi o Mercedes.
```{r}
fviz_ca_biplot(res.ca,repel=TRUE)+theme_bw() 
```

Es por este motivo, que factores como pueden ser manufacturer o engineSize pueden no resultar determinantes a la hora de explicar nuestro target numérico price.


\newpage
# Multiple Correspondence Analysis
A continuación vamos a proceder a realizar el MCA. Para ello, en primer lugar vamos a seleccionar los individuos que hemos considerado como multivariant outliers para añadirlos como suplementarios.
También se ha considerado la eliminación de la variable engineSize ya que si se incluye genera errores.
También se han suprimido los factores model y year. Model se ha quitado ya que tiene demasiados valores y acaba no siendo explicativo. Year se ha eliminado porque guarda demasiada relación con f.age.
Se añaden como variables suplementarias los factores f.price y Audi, además de la variable price, ya que son nuestros targets.
```{r}
llvout<-which(df$mout=="YesMOut");length(llvout)
res.mca<-MCA(df[,c("f.price","Audi",vars_cat[c(3:4,6,8:11)],"price", vars_num) ], quali.sup=c(1,2),quanti.sup=10:14 , ind.sup=llvout) # Quitamos engineSize porque si no no funciona y f.price para que no salga duplicada
```

Vamos a aplicar el criterio de Kaiser para determinar el número de dimensiones relevantes para continuar el análisis MCA. En este caso, como los eigenvalues no están normalizados, nos quedaremos con todas las dimensiones que tengan un eigenvalue mayor que la media de todos los eigenvalues.
```{r}
length(which(res.mca$eig[,1] > mean(res.mca$eig[,1])))
```

A continuación podemos ver los eigenvalues de las dimensiones seleccionadas, los porcentajes de varianza que acumulan y las varianza que hay acumulada hasta esa dimensión. Podemos ver que para el caso de la dimensión 7, que es la que nos ha indicado el criterio de Kaiser, se ha aglutinado cerca de un 60% de la variabilidad.
```{r}
res.mca$eig[1:7,]
```

En el siguiente gráfico podemos ver de una manera más visual la variabilidad acumulada para cada dimensión:
```{r}
fviz_eig(res.mca)
```

## Análisis segun las variables
### Factores
Si analizamos el estadístico cos2, podemos ver la correlación que existe entre los factores y las componentes que se han creado. Podemos ver que para la Dim 1, el valor más relevante es f.age-[1,2] o f.gae-[+4] y para la segunda dimensión estaría f.Trans-Manual.
```{r}
res.mca$var$cos2
```

En el siguiente gráfico se pueden apreciar los valores de cos2 de un modo más visual:
```{r}
fviz_cos2(res.mca, choice = "var", axes = 1:2)+theme_bw()+theme(axis.text.x = element_text(angle=90))
```

Por último, si echamos un vistado a eta2, podemos ver la relevancia que tienen los factores, no solo los valores que estos toman.
```{r}
res.mca$var$eta2
```
Podemos ve que para la primera dimensión, los factores más relevantes son f.age y f.miles, mientras que para la segunda serían manufacturer y transmission.

En el siguiente gráfico, podemos ver la contribución que tienen las variables para las dos primeras dimensiones generadas por el MCA, y que recordemos, acumulan cerca del 25% de la variabilidad.
```{r}
fviz_mca_var(res.mca, col.var="contrib",repel=TRUE,labelsize = 2)+
    scale_color_gradient2(low="green", mid="blue", 
    high="red", midpoint=0.75)+theme_bw()
```

### Variables numéricas

Vamos a echar un vistazo a las variables numéricas. En la siguiente salida podemos observar la correlación que existe entre las variables numéricas originales y las componentes que se han obtenido con el MCA.
```{r}
res.mca$quanti
```
Podemos destacar la fuerte correlación de las variables age y mileage con la primera componente, y de price con la segunda.

Como curiosidad, podemos mencionar que el hecho de que para la primera dimensión age y mileage aparezcan tan correlacionadas puede ser debido al hecho de sus factores derivados son los que más relevancia adquieren en esta primera dimension.

\newpage
# Clustering Jerárquico desde MCA

Vamos a proceder a volver a realizar el clustering jerárquico pero esta vez lo vamos a lanzar desde el MCA en lugar del PCA.
Vamos a añadir el número de componentes que hemos determinado durante el análisis MCA a partir del criterio de Kaiser.
```{r}
res.mca<-MCA(df[,c("f.price","Audi",vars_cat[c(3:4,6,8:11)],"price",vars_num) ],quali.sup=c(1,2),quanti.sup=c(10:14) , ind.sup=llvout, ncp=7)
res.hcmc<-HCPC(res.mca,nb.clust=-1,order=TRUE)
```
Como podemos ver en la salida, en este caso, a partir del parámetro nb.clust=-1 que como hemos mencionado anteriormente, automatizaba la selección del número optimo de clusters, se han generado 4 clusters.

Sin embargo, si tenemos en cuenta el criterio de Kaiser, podemos ver que, en este caso, este número de componentes no son suficientes. Según Kaiser, deberíamos tomar 7 componentes.
```{r}
length(which(res.hcmc$call$t$res$eig[,1] > mean(res.hcmc$call$t$res$eig[,1])))
```

En el siguiente plot podemos ver la ganancia de inercia que se produce en nuestro modelo de clusterización jerarquica.
Si aplicamos la regla de elbow, podemos determinar que el número óptimo de clusters sería alrededor de 6.
```{r}
plot(res.hcmc$call$t$inert.gain[1:20])
```

Por lo tanto, y resumiento, nb.clust nos da 4 componentes, elbow nos da 6 y Kaiser nos da 7. Nos quedaremos con el de Kaiser porque es el más específico.

```{r}
res.hcmc<-HCPC(res.mca,nb.clust=7,order=TRUE)
```

A continuación podemos ver inercia acumulada en las 7 primeras componentes.
```{r}
(res.hcmc$call$t$within[1]-res.hcmc$call$t$within[1:7])/res.hcmc$call$t$within[1]
```

Crearemos una nueva variable en nuestro df para guardar en que cluster se han asignado los individuos.
```{r}
df$claHCMC<-7
df[row.names(res.hcmc$data.clust),"claHCMC"]<-res.hcmc$data.clust$clust
df$claHCMC<-factor(df$claHCMC)
levels( df$claHCMC ) <- paste0( "f.claHCMC-",levels( df$claHCMC ))
table(df$claHCMC)
```

## Análisis según las variables
### Factores
En primer lugar, vamos a analizar el estadístico chi-squared para determinar que factores son las que más determinan las diferencias entre clusters:
```{r}
res.hcmc$desc.var$test.chi2
```

Si profundizamos un poco más podemos ver caracterizar un poco los clusters según los valores de estos factores. 
<He omitido la salida porque ocupa mucho, pero podemos ver algunas de las conclusiones abajo>
```{r}
#res.hcmc$desc.var$category
```
Podemos ver, por ejemplo, que el cluster 1 esta compuesto por coches muy nuevos (f.age=f.age-[1,2] -> Mod/Cla 96.6631908) y con poco kilometraje (f.miles=f.miles-[0.001,5.81] -> Mod/Cla 69.9687174). Por el contrario, practicamente no hay coches viejos (f.age=f.age-(+4) -> Mod/Cla 0.1042753) o con precios bajos (f.price=f.price-[899,1.1e+04] -> Mod/Cla 0.00000000).

Contrariamente, en el cluster 7, se acumulan los vehículos con más de 4 años, Diesel y con mpg elevados.

### Variables numéricas
En los que se refiere a las variables cuantitavitas:
```{r}
res.hcmc$desc.var$quanti.var
```
Podemos ver que la que mejor explica la separación entre clusters es age, mientras que price parece ser la menos explicativa.

En este apartado, cabe destacar que todas las variables tienen factores derivados que se han usado para generar el clustering. Sin embargo, hay que tener en cuenta que en el caso de nuestro target price, su discretización solo se ha añadido como variable suplementaria, de modo que no ha tenido influencia en la clusterización, hecho que explica que aparezca como la menos representativa.

Si analizamos más en profundidad, podemos ver las distribuciones que toman cada una de las variables cualitativas dentro de los distintos clusters.

```{r}
res.hcmc$desc.var$quanti
```
Como se ha comentado con anterioridad según la descripción a partir de los factores, podemos ver que en el primer cluster price es más alto, mientras que mileage, age o mpg son signitivamente más bajos que en el resto del df. De modo que en este cluster aparecen los coches más nuevos y caros.

Para el caso del cluster 2, podemos ver como aparecen coches algo más caros que la media, pero sin llegar al nivel del primer cluster.

En el caso del cluster 3, podemos ver como price pasa a ser un factor no tan relevante en favor de age y mpg.

Asimismo, podemos ver como en el cluster 7, se acumulan vehiculos más viejos y con más kilometraje, pero también más baratos.


## Análisis según las componentes del MCA
Vamos a proceder a analizar la clusterización a apartir de los ejes generados a partir del MCA.

En primer lugar, podemos ver la relevancia que han tenido las distintas componentes del MCA para generar la clusterización:
```{r}
res.hcmc$desc.axes$quanti.var
```
Podemos ver como la dimensión 1 ha tenido la mayor relevancia, mientras que la dimensión 6 ha sido la menos determinante.

Por último, podemos ver algunas estadísticas sobre como se distribuyen las coordenadas del MCA para cada individuo en los diferentes clusters:
```{r}
res.hcmc$desc.axes$quanti
```

## Análisis según individuos

En las siguientes salidas podemos ver los paragons del primer cluster.
```{r}
res.hcmc$desc.ind$para
```

Curiosamente, entre los paragons del primer cluster, aparecen vehículos VW semi-automáticos Diesel. Todos tienen entre 1 y 2 años y consumos y kilometrajes muy bajos.
```{r}
summary(df[c("40410","44910","45035","45038","45440"),])
```

En la siguiente salida podemos ver los individuos característicos de cada cluster:
```{r}
res.hcmc$desc.ind$dist
```

Si nos fijamos, en este caso, los infivi
```{r}
summary(df[c("5388", "7854", "10322", "10323", "10482"),])
```
Si recogemos los datos de los paragons del primer cluster, podemos ver como todos los vehículos son Audi, de cambio automático y gasolina. También tienen consumos muy bajos y tiene entre 1 y 2 años. 


# K-Means Clustering desde MCA

Por último, vamos a realizar un clustering con el algoritmo de K-Means a partir del MCA.
```{r}
ppcc <- res.mca$ind$coord[,1:7];
kc<-kmeans(dist(ppcc),7)
```

En primer lugar, vamos a analizar el porcentaje de distancias que se acumulan con la suma de distancias entre clusters respecto al total.
```{r}
kc$betweenss/kc$totss
```
Podemos ver que es de un 61%, lo que a priori es peor que en la clusterización a partir de ACP.

En el caso de las distancias dentro de los clusters, estas suman un 39%.
```{r}
kc$tot.withinss/kc$totss
```

## Profiling de la clusterización

Vamos a analizar las cualidades de los distintos clusters que se han creado.
```{r}
df[-ll, "claKMMCA"] <- kc$cluster
df$claKMMCA <- factor(df$claKMMCA)
km.catdes<-catdes(df[,c("claKMMCA",vars_num,vars_cat, "price")],1)
```

```{r}
km.catdes$quanti[1]
```
Para el primer cluster, podemos ver como la variable más determinante ha sido tax, en concreto, se agrupan los individuos que han pagado más impuestos. Podemos ver como el consumo de estos individuos es más alto. Puede que hablemos de vehículos ecológicos con subvenciones.

```{r}
km.catdes$quanti[2]
```
Para el segundo cluster, podemos ver como la variable más deterinante es el precio, que es muy inferior a la media. Podemos destacar también que los vehículos son más viejos y con más kilometraje.

```{r}
km.catdes$quanti[3]
```
En lo que se refiere al tercer cluster, podemos ver como la variable más determinante es el consumo. En concreto aparecen vehículos con un consumo inferior. Estos vehículos también son más nuevos y tienen un kilometraje inferior a la media.


```{r}
km.catdes$quanti[7]
```
Pasando directos al cluster 7, podemos ver como en este caso, aparecen vehículos más baratos y con menor kilometraje e impuestos que la media, aunque en general no están muy alejados del centroide del dataset.

Con estos dos resultados, se puede concluir que la clusterización desde el ACP es de mayor calidad que la generada a partir del ACM. 

Este hecho es aún más evidente si lo representamos de manera gráfica:
```{r}
fviz_cluster(kc, data=ppcc)
```

Lamentablemente, al solo poder representar los clusters en 2 dimensiones, el gráfico resultante es practicamente incomprensible, de modo que vamos a realizar el mismo proceso pero solo aportando las coordenadas pertenecientes a las 2 primeras componentes del MCA, que son las que más variabilidad acumumulan (25%).

<Solo por motivos de visualización. Si pudieramos representar las 7 dimensiones veriamos como las clusterización SÍ que tiene sentido>

```{r}
ppcc <- res.mca$ind$coord[,1:2];
kc<-kmeans(dist(ppcc),7)
fviz_cluster(kc, data=ppcc)
```
En este caso si que podemos ver bien diferenciados los distinos clusters.

Si analizamos las distancias, podemos ver como en este caso, se crean clusters más diferenciados entre si, pero con individuos más similares. Esto es debido a que solo se están usando 2 dimensiones y la variabilidad qu se acumula es muy baja.
```{r}
kc$betweenss/kc$totss;kc$tot.withinss/kc$totss
```
Podemos ver que la distancia entre clusters acumula un 82% del total, mientras que las distancias dentro de los clusters acumulan solo un 17%. Seria un resultado bastante bueno si con las dos dimensiones que estamos representado acumuláramos mayor variabilidad.

Por último, volvemos a poner el gráfico generado a partir del clustering jerárquico con MCA para comparar las distintas clusterizaciones que se han llevado a cabo segun clustering jerárquico y Kmeans.
```{r}
plot.HCPC(res.hcmc, choice="map")
```

```{r, echo=FALSE, results=FALSE, include=FALSE}
vars_res<-c("price","Audi")
vars_dis<-c("model", "year", "transmission", "fuelType", "engineSize", "manufacturer", "f.price", "f.miles", "f.mpg", "f.tax", "f.age")
vars_con<-c("mileage", "tax", "mpg", "age")

c(vars_res, vars_dis,vars_con, "mout")
summary( df[ , c(vars_res, vars_dis, vars_con, "mout") ] )
```

## Variables explicativas numéricas
Primero de todo, vamos a empezar aplicando una corrección para los valores de nuestras variables numéricas eliminando los valores 0 para poder eviar algunos errores que podrían salir a posteriori.
```{r, results=FALSE}
vars_con
ll<-which(df$age==0);ll
df$age[ll]<-0.5

ll<-which(df$tax==0);ll
df$tax[ll]<-0.5

ll<-which(df$mpg==0);ll
df$mpg[ll]<-0.5

ll<-which(df$mileage==0);ll
df$mileage[ll]<-0.5
```

# Modelo de regresión lineal

## Variables numéricas
Planteamos nuestro primer modelo basado en las variables numéricas. Con este modelo, pretendemos plantear una regresión lineal que tenga como target la variable price, y que use como variables explicativas mileage, tax, mpg y age.
```{r}
m1<-lm(price~mileage+tax+mpg+age,data=df)
summary(m1)
```
En el summary que acabamos de mostrar, podemos apreciar varias cosas:

En primer lugar, podemos ver los errores residuales que se generan en el modelo.

También podemos ver los coeficientes que se plantean para las difernetes variables del modelo así como el término independiente (Intercept)  5.476e+04. Podemos ver como a priori, los coeficientes para todas las variables son significativamente distintos a 0.

Con el valor R-squared, podemos apreciar también que el modelo este modelo explica un 51% de la variabilidad de la variable price.

Por último, podemos ver también como el F-statistic nos indica que nuestra hipótesis nula de que todos los coeficientes sean iguales a 0 se puede rechazar. 

Si analizamos los valores de vif (variable inflation factor), podemos ver que los más altos corresponden a las variables de mileage y age, cercanos a 3. Estos valores nos indican la co-linealidad de las variables, y empezarían a ser preocupantes cuando se acercan al 5, de modo que de momento son correctos.
```{r}
vif(m1)
```

En los siguientes gráficos, podemos apreciar algunas de las características del modelo.
```{r}
par(mfrow=c(2,2));
plot(m1,id.n=0)
```
\newline
En el gráfico de Residuals vs. Fitted podemos apreciar como no existe homocedasticidad en el modelo, ya que los residuos estan distribuidos de manera heterogénea.

En el gráfico Normal Q-Q podemos ver como el modelo no presenta normalidad, ya que los residuos estandarizados se alejan de la recta que marca la normalidad.

En el gráfico Scale-Location, podemos volver a apreciar la heterosticidad del modelo, analizando la raiz de los residios estandarizados.

En el último gráfico, el de Residuals vs Leverage, podemos apreciar como a priori no parece que existan observariones influyentes, ya que ninguno de los puntos que se muestran tiene una distancia de Cook que indique sobre-influencia.

```{r, results=FALSE, include=FALSE}
library(MASS)
```

A continuación, vamos a determinar si alguna de las variables que hemos introducido en el modelo anterior requiere algún tipo de transformación para lograr un mejor ajuste del modelo.
```{r}
par(mfrow=c(1,1))
boxcox(price~mileage+tax+mpg+age,data=df)
```
En el gráfico podemos ver como el intervalo de lambda que aparece es cercano al 0, hecho que indicaría la necesidad de transformar nuestra variable target price a una escala logarítmica.

Vamos a volver a evaluar la co-linealidad de nuestra variable target con las dependientes habiendo aplicado la transformación logaritmica.
```{r}
boxcox(log(price)~mileage+tax+mpg+age,data=df)
```
Como podemos ver, en este caso aparece un máximo en la función cerca de lamda = 2, de modo que sería recomendable aplicar una transformación cuadrática al target:
```{r}
boxcox(log(price)^2~mileage+tax+mpg+age,data=df)
```

Vamos a proceder a plantear un segundo modelo que incluya esta transformación.
```{r}
m2<-lm(log(price)~mileage+tax+mpg+age,data=df)
summary(m2)
```
Para este modelo, podemos apreciar como todos los coeficientes siguen siendo estadísticamente diferentes a 0 según el p-valor que se muestra en la última columna. Cabe destacar que para el caso de la variable tax, el p-valor ha subido y es cercano a 0.1.

Podemos ver también como el valor R-squared ha aumentado a 0.58, indicando que este nuevo modelo acumula más explicabilidad de nuestro target.

También podemos ver como los valores de vif se mantienen similares a los que aparecian en el modelo anterior, indicando que no parece existir co-linealidad entre las variables.
```{r}
vif(m2)
```

Observemos algunos gráficos para analizar este nuevo modelo que hemos planeado.
```{r}
par(mfrow=c(2,2));
plot(m2,id.n=0);
par(mfrow=c(1,1))
```
Podemos ver como según el primer gráfico, el modelo ha adquirido homocedasticidad, siendo la distribución de los residuos más homogénea.

En lo que se refiere a la normalidad, podemos ver como este modelo presenta más normalidad que el anterior, al menos en lo que se refiere a los quantiles superiores. Sin embargo, para los cuantiles inferiores, aún aparece cierta distancia con la recta que describe la normalidad.

Por último, para el caso de la sobre-influencia en el modelo, podemos ver un resultado similar al del anterior modelo.

Vamos a proceder a realizar un análisis más en profuncidad del modelo.

En los siguientes gráficos podemos ver como se distribuyen los residuos.
```{r}
residualPlots(m2,id=list(method=cooks.distance(m2),n=10))
```
Algunos casos a destacar son el de mileage, donde se puede apreciar una cierta acumulación en los valores bajos a la vez que aparecen puntos que se alejan de la nube y de la curva para valores más altos.

También podemos destacar que en el caso de age, que es una variable que originalmente generamos a partir de la variable year y se puede apreciar su distribución en columnas, aparece una nube de puntos cerca del centro, probablemente debido a la imputación a partir de PCA que se realizó en la primera entrega. Podemos ver algo parecido pero no tan evidente para la variable tax.

Con la siguiente función procederemos a ver el ajuste del modelo con los datos.
```{r}
marginalModelPlots(m2)
```
Podemos ver como el ajuste para el caso de la variable mileage o age parecen casi perfectos, mientras que para las varibales mpg y tax existe una cierta desviación entre las curvas roja y azul, que nos indican que hay falta de linealidad entre la variable target y las variables expliactivas.

En el caso de los Fitted values, las rectas se ajustan a la perfección.

Aplicando la función de boxTidwell podemos determinar las transformaciones que deberíamos aplicar a nuestro modelo para mejorarlo.
```{r}
#boxTidwell(log(price)^2~mileage+tax+age+mpg,data=df[!df$mout=="YesMOut",], verbose=TRUE)
```
Sin embargo, cuando ejecutamos la función pasando como entrada el modelo que habíamos planteado, nos encontramos que la función falla. Aplicando el modo verbose, podemos ver como el motivo del fallo es la tendencia a +infinito del exponenete de la variable mpg.

Como no sé como proceder, vamos a realizar algunos tests:

En primer lugar, vamos a crear un nuevo modelo sin la variable mpg y lo vamos a evaluar con la función boxTidwell para determinar las transformaciones a aplicar a nuestras variables explicativas (aunque hemos tenido que aumentar el número máximo de iteraciones).
```{r}
m3<-lm(log(price)~mileage+tax+age, data=df)
boxTidwell(log(price)~mileage+tax+age, data=df[!df$mout=="YesMOut",], max.iter=100)
```
Como podemos ver, este nuevo modelo que excluye la variable mpg pierde explicabilidad, pero nos permite ejecutar la función boxTidwell para determinar las transformaciones que deberíamos aplicar a nuestras variables.

Vamos a aplicar las transformaciones que aparecen con la función boxTidwell para crear un nuevo modelo.
```{r}
m4<-lm(log(price)~sqrt(mileage)+poly(tax,4)+age, data=df)
summary(m4)
#boxTidwell(log(price)~sqrt(mileage)+poly(tax,4)+age, data=df[!df$mout=="YesMOut",], verbose=TRUE)
```
Volviendo a aplicar la función de boxTidwell, podemos ver como, en este caso vuelve a fallar debido a que algunos de los coeficientes de los monómios que se han generado con la función poly(tax,4) son negativos.
Sin embargo, el modelo que excluye la variable mpg, aún aplicando todas las transformaciones recomendadas, tiene un valor de R-squared inferior al original.

Si ejecutamos el test de Clarke que nos permite analizar modelos no anidados, (lo he encontrado por internet) podemos ver que, aparentemente, el modelo original, que incluye la variable mpg pero ninguna transformación a parte de la logaritmica para el target, es mejor que el modelo que excluye esta variable pero incluye las transformaciones.
```{r}
library(games)
clarke(m2, m4)
```

Vamos a probar incluyendo la variable mpg en el modelo que ya habíamos planteado aplicando las transformaciones que aparecían con la función boxTidwell.
```{r}
m5<-update(m4,~.+mpg)
summary(m5)
```
En este caso, se puede ver como la incorporación de la variable mpg sí que aumenta la explicabilidad del modelo, generando un modelo más completo.

Además, aplicando la función anova, podemos ver como se rechaza la hipótesis de equivalencia, de modo que el nuevo modelo es mejor.
```{r}
anova(m4,m5)
```

Sin embargo, si lo comparamos con el original:
```{r}
clarke(m2,m5)
```
Podemos ver como el test de clarke para modelos no anidados determina que el primer modelo es preferible.

De modo que seguiremos avanzando con el segundo modelo que hemos planteado, que incluye la variable mpg pero no incluye ninguna transfromación a parte de la del target price.

Vamos a proceder a observar algunos gráficos para analizar este modelo:
```{r}
par(mfrow=c(2,2));
plot(m2,id.n=0);
par(mfrow=c(1,1))
```
Podemos ver como según el primer gráfico, el modelo ha adquirido homocedasticidad, siendo la distribución de los residuos más homogénea.

En lo que se refiere a la normalidad, podemos ver como para este modelo tiene más normalidad que el anterior, al menos en lo que se refiere a los cuantiles superiores. Sin embargo, para los cuantiles inferiores, aún aparece cierta distancia con la recta que describe la normalidad.

Por último, para el caso de la sobre-influencia en el modelo, podemos ver un resultado similar al del anterior modelo.

Vamos a proceder a realizar un análisis más en profuncidad del modelo.

En los siguientes gráficos podemos ver como se distribuyen los residuos.
```{r}
residualPlots(m2,id=list(method=cooks.distance(m2),n=10))
```
Algunos casos a destacar son el de mileage, donde se puede apreciar una cierta acumulación en los valores bajos, a la vez que aparecen puntos que se alejan de la nube y de la curva para valores más altos.

También podemos destacar que en el caso de age, que es una variable que originalmente generamos a partir de la variable year tiene una distribución en columnas. Aparece una nube de puntos cerca del centro, probablemente debido a la imputación a partir de PCA que se realizó en la primera entrega. Podemos ver algo parecido pero no tan evidente para la variable tax.

Con la siguiente función procederemos a ver el ajuste del modelo con los datos.
```{r}
marginalModelPlots(m2)
```
Podemos ver como el ajuste para el caso de la variable mileage o age parecen casi perfectos, mientras que para las varibales mpg y tax existe una cierta desviación entre las curvas roja y azul.

En el caso de los Fitted values, las rectas se ajustan casi a la perfección.

Vamos a proceder a volver a generar el modelo excluyendo los multivariant outliers.
```{r}
m6<-update(m2, data=df[!df$mout=="YesMOut",])
summary(m6)
```
En el caso del R-squared, podemos ver como este ha bajado, aunque infimamente, indicando que el modelo anterior aglutinaba más variabilidad de nuestro target.

Vamos a proceder a estudiar la validez de nuestro tercer modelo.
```{r}
Anova(m6)
```
En este test podemos ver que todas las variables que aparecen son significaticas, de modo que ninguna de estas variables es prescindible.

Con la función de VIF podemos ver como, los vifs asociados a mileage y age son mayores que 3, pero de momento no debería importarnos mucho.
```{r}
vif(m6)
```

Vamos a analizar los gráficos del modelo:
```{r}
par(mfrow=c(2,2))
plot(m6,id.n=0)
```
En el de Residuals vs Fitted y Scale-Location podemos ver como el modelo presenta bastante homocedasticidad.

En el de Normal Q-Q, similarmente al m2, podemos apreciar como en los cuantiles superiores aparece normalidad, mietras que para los inferiores, la normalidad de nuestro modelo se aleja de la recta debido a algunas observaciones extrañas.

Por último, en el gráfico de Residuals vs Leverage, podemos ver como para este modelo siguen sin aparecer puntos con una distancia de Cook relevante, de modo que no parece existir sobre-influencia de ninguna observación.

En los siguientes gráficos podemos apreciar como las rectas se ajustan bastante, excepto en el caso de la variable mpg, a la que, dados los problemas que aparecen con la tendencia a infinito de su exponente en las iteraciones de la función boxTidwell, no podemos determinar la transformación que se le debería aplicar para mejorar el modelo.
```{r}
par(mfrow=c(2,3))
residualPlots(m6,id=list(method=cooks.distance(m6),n=10))
```
Podemos apreciar que hay tres individuos que aparecen constantemente fuera de las nubes de puntos, los 20688, 19848 y 40774.

Vamos a proceder a eliminar los elementos que aparecían en los plots anteriores constantemente alejados de las nubes de puntos así como los multivariant outliers.
```{r}
df2 <- df[!df$mout=="YesMOut",]
df2 <- df2[row.names(df2)!="19848",]
df2 <- df2[row.names(df2)!="40774",]
df2 <- df2[row.names(df2)!="20688",]
```

Procederemos a replantear el modelo excluyendo las observaciones que hemos comentado previamente.
```{r}
m7<-update(m6,data=df2)
summary(m7)
```
Se puede apreciar como aumentan la explicabilidad pero no hay grandes cambios en la relevancia de los coeficientes.

En los siguientes plots podemos ver como este pequeño cambio en el dataset hemos eliminado los indiviuos que constantemente se desviaban de las nubes de puntos. Este hecho se puede ver especialmente en el gráfico Normal Q-Q, donde hay un mejor ajuste a la recta para los cuantiles inferiores.
```{r}
par(mfrow=c(2,2))
plot(m7,id.n=0)
```

Si nos fijamos en los gráficos de los residuos, podemos ver como siguen existiendo algunos desajustes en las rectas, sobretodo para las variables mpg y age.
```{r}
par(mfrow=c(2,3))
residualPlots(m7,id=list(method=cooks.distance(m7),n=10))
```

Si realizamos el test de Breusch-Pagan contra la heteroscedasticidad de nuestro modelo, obtenemos un p-valor de 0.0005, de modo que podemos rechazar la H_0 y confirmar que nuestro modelo es homocedástico.
```{r}
library(lmtest)
bptest(m7)
```

Vamos a proceder a mostrar los boxplots de los valores R-student, Hat y distancias de Cook de las observaciones del modelo.
```{r}
par(mfrow=c(1,3))
Boxplot(abs(rstudent(m7)),id=list(labels=row.names(df2)))
Boxplot(abs(hatvalues(m7)),id=list(labels=row.names(df2)))
Boxplot(cooks.distance(m7),id=list(labels=row.names(df2)))
```
Con estos gráficos podemos detectar los valores para los cuales se rompe la cadena de puntos y podemos categorizar como outliers.
```{r}
stu_out <- which(abs(rstudent(m7))>3.7);
cook_out <- which(abs(cooks.distance(m7))>0.0065);
hat_out <- which(abs(hatvalues(m7))>0.007);

outs<-unique(c(stu_out,cook_out,hat_out));outs
```

Si analizamos el gráfico de influencias, podemos ver como no existe una distribución aglomerada, tal vez en los individuos con valores de Hat muy bajos, pero en general existe basante dispersión.
```{r}
par(mfrow=c(1,1));
outs2 <- influencePlot(m7, id=list(n=10));
outs2 <- labels(outs2)[[1]];
outs2 <- as.numeric(outs2);
outs3 <- unique(c(outs,outs2));outs3
```

Vamos a terminar este análisis generando un modelo excluyendo los individuos que hemos detectado como outliers.
```{r}
m8 <- update(m7, data=df2[-outs3,])
summary(m8)
```
Vemos que la explicabilidad del modelo final es del 57,47%.


## Factores

Vamos a empezar añadiendo un solo factor al modelo. En este caso empezaremos con los factores que determinamos que eran más influyentes en el análisis MCA de la entrega anterior. Los tres factores más relevantes eran f.price, transmission y fuelType. No añadiremos f.price ya que es un factor generado a partir de nuestra variable target.

De momento empezamos añadiendo fuelType.
```{r}
m10<- update(m8, ~.+fuelType)
summary(m10)
```
Como podemos ver ya a simple vista, el R-squared del modelo ha aumentado significativamente, indicando mayor explicabilidad.

Si analizamos los vif, vemos que no ha habido cambios significativos respecto al modelo anterior. Los vifs se mantinenen en valores inferiores a 5.
```{r}
vif(m10)
```

Con el test anova podemos ver como todas las variables mantienen su significatividad.
```{r}
Anova(m10)
```

Si analizmos los plots del modelo, podemos ver como el modelo parece haber perdido homocedasticidad y normalidad en los extremos, y la distribución de las distancias de Cook es algo peculiar, generando diversas acumulaciones de puntos.
```{r}
par(mfrow=c(2,2))
plot(m10,id.n=0)
```

Vamos a proceder a incorporar también el factor transmission.
```{r}
m11 <- update(m10, ~.+transmission)
summary(m11)
```
Con el aumento del R-squared, podemos apreciar un nuevo aumento en la explicabilidad del modelo. 

El test de anova nos indica que el nuevo modelo es preferible al anteior.
```{r}
anova(m10,m11)
```

Si analizamos los plots de este modelo, podemos ver como no aparecen cambios significativos. Tal vez podemos apreciar algo más de normalidad, pero la homocedasticidad y las distancias de Cook no parecen haber cambiado mucho.
```{r}
par(mfrow=c(2,2))
plot(m11)
```

En lo que se refiere a los vifs, no hay cambios significativos, indicando que no aparece co-linealidad en las variables.
```{r}
vif(m11) 
```

Si analizamos los plots de los residuos, para las variables numéricas no apreciamos grandes cambios, 
```{r}
par(mfrow=c(1,1))
residualPlots(m11,id=list(method=cooks.distance(m11),n=10))
```

## Interacciones

### Interacciones entre factores

Vamos a proceder a incorporar la interacció entre los factores transmission y fuelType. 
```{r}
m13 <- update(m11, ~.+transmission*fuelType)
summary(m13)
```
Podemos ver como el R-squared apenas aumenta y se mantiene la explicabilidad de las variables.

Si realizamos el test de anova, podemos ver como, a pensar de que a partir del summary los modelos parecen similares, no lo son, y en realidad el nuevo modelo es mejor que el anterior.
```{r}
anova(m11,m13)
```

### Interacciones Factor-Numéricas
Vamos a probar a añadir las interacciones de la variable numérica age y el factor transmission. Podemos ver como los dos modelos no son equivalentes, y según el test de anova, el nuevo modelo es más completo.
```{r}
m14 <- update(m13, ~.+age*transmission)
anova(m13,m14)
```

Si analizamos los plots del modelo, podemos ver como existe homocedasticidad y aparece bastante normalidad.
```{r}
par(mfrow=c(2,2))
plot(m14, id.n=0)
```

Si analizamos los plots de los residuos, podemos ver como el ajuste de las regresiones entre el modelo y los datos es muy preciso.
```{r}
marginalModelPlots(m14)
```
Podemos ver claramente fuerte relación entre la variable mpg y nuestro target numérico.

```{r}
library(effects);
plot(allEffects(m14))
```
Además, si nos fijamos en la interaccions de age y transmission, podemos ver como para todos los tipos de transmisión se respeta la relación inversa con el precio: cuantos más años tiene el coche, más barato es, y viceversa. Por el contrario, en el caso de fuelType, podemos ver como los coches de gasolina son más baratos que los diesel o híbridos.

Si hacemos una rápida búsqueda en internet (https://www.motor.mapfre.es/consejos-practicos/consejos-para-ahorrar/diesel-o-gasolina/) podemos ver como esto este hecho cuadra con la realidad.

El problema aparece cuando intentamos aplicar la función vif para comprobar la no co-linealidad de las variables del modelo, ya que salta un error que nos indica que si que hay variables co-lineales. Sin embargo, si realizamos un análisis de las varianzas, todas las variables parecen significativas. Las que menos significación adquieren son tax y la interacción entre age y transmission.
```{r}
#vif(m14)
Anova(m14);
```

Vamos a proceder a aplicar la función step para tratar de eliminar esta co-linealidad.
```{r}
m15 <- step( m14, k=log(nrow(df2)))
#vif(m15)
```

Si realizamos el test de anova, podemos ver como el nuevo modelo no es equivalente al anterior, y de hecho, sigue apareciendo co-linealidad en las variables, de modo que el m14 es preferible.
```{r}
anova(m15,m14)
```

Si aplicamos la función alias, podemos ver que esta colinealidad parece generada por la aparición de las variables transmission y fuelType. Supongo que es debido a que los coches híbridos suelen tener transmisión automática, de modo que el conjunto Hybrid:SemiAuto queda vacío, mientras que hybrid y hybrid&Automatic son el mismo conjunto.
```{r}
alias(m14)
```

```{r}
summary(m14)
```
Finalmente, si echamos un último vistazo al modelo, podemos ver que hemos conseguido aglomerar una explicabilidad del 80%.

# Modelo de regresión Binaria

Para plantear el modelo de regresión binaria, vamos a proceder primero a separar nuestro dataframe en dos subconjuntos de entrenamiento y de validación.
```{r}
llwork <- sample(1:nrow(df2),round(0.80*nrow(df2),0))

df_train <- df2[llwork,]
df_test <-df2[-llwork,]
```

## Variables numéricas

Vamos a empezar el proceso de generación del modelo de regresión binaria incorporando las variables numéricas y planteando un modelo incial.
```{r}
m20<-glm(Audi~mileage+tax+mpg+age,family="binomial",data=df_train)
summary(m20)
```
Como podemos ver, la variable más significativa de nuestro modelo incial parece ser mpg, mientras que mileage o tax parecen no ser significativas.
También podemos ver como el valor AIC es 3810.1, nuestro objetivo será reducirlo.

Si miramos los valores vif de nuestro modelo, podemos ver como no reflejan co-linealidad entre las variables.
```{r}
vif(m20)
```

Vamos a plantear un nuevo modelo que incluya solo las variables mpg y age, que son las que aparecían como significativas en el modelo anterior.
Si realizamos el test anova de los dos modelos, podemos ver como los modelos parecen equivalentes, de modo que será preferible trabajar con el más sencillo.
```{r}
m21 <- glm(Audi~ mpg + age,family="binomial",data=df_train)
anova(m21,m20,test = "LR");
```

Probaremos a añadir los polinomios de segundo grado de las variables del modelo que acabamos de generar.
El test de anova nos vuelve a indicar que los modelos son equivalentes, de modo que continuaremos trabajando con el más simple.
```{r}
m22 <- glm(Audi~ poly(mpg,2) + poly(age,2), family="binomial", data=df_train)
anova(m21,m22, test = "LR")
```

Finalmente, este es el modelo con el que continuaremos trabajando, ya que es el más sencillo y equivalente a otros modelos más complejos que hemos planteado.
```{r}
summary(m21)
```
Podemos ver como el AIC de este modelo es 3807.8.


## Factores

Vamos a proceder a añadir los factores que resultaron más significativos en el análisis MCA de la segunda entrega.

Como el factor engineSize tiene muchos niveles, vamos a proceder a transformala para que adquiera solo 3 niveles y poder simplificar el modelo.
```{r}
df_train$engineSize_f <- as.integer(df_train$engineSize)
par(mfrow=c(1,1))
hist(df_train$engineSize_f)
quantile(df_train$engineSize_f, c(0.3333333,0.6666666,1))
df_train$engineSize_f <- factor(cut(df_train$engineSize_f, breaks = c(0,8,9,20)))
df_test$engineSize_f <- as.integer(df_test$engineSize)
df_test$engineSize_f <- factor(cut(df_test$engineSize_f, breaks = c(0,8,9,20)))
table(df_train$engineSize_f)
```

Añadimos los factores fuelType y transmission al modelo. Podemos ver como añadiendo estos dos factores hemos conseguido una reducción del AIC. Además, con la función vif podemos ver que no hay aparente co-linealidad entre las variables explicativas del modelo.
```{r}
m24 <- update(m21, ~.+fuelType+transmission)
summary(m24)
vif(m24)
```

Si analizamos la varianza del modelo, podemos ver como todas las variables de nuestro modelo son sifnificativas.
```{r}
Anova(m24, test="LR")
```

Con el test anova podemos ver como los modelos no son equivalentes, de modo que nos quedaremos con el que incluye los factores, ya que presenta un AIC inferior.
```{r}
anova(m21,m24, test="LR")
```

Ahora añadiremos el factor derivado de engineSize que hemos generado antes. Podemos ver que este nuevo modelo vuelve a presentar un AIC inferior al del anterior. Además, si usamos la función vif podemos ver como no existe co-linealidad entre las variables.
```{r}
m25 <- update(m24, ~.+engineSize_f)
summary(m25)
vif(m25)
```

Si observamos el análisis de la varianza, vemos que todos los componentes de nuestro modelo son representativos.
Por otro lado, no podemos usar las funciones anova ni AIC para comparar ambos modelos, ya que al incluir el factor EngineSize, se han generado algunos missings en los datos que imposibilitan la comparación.
```{r}
Anova(m25, test="LR")
#anova(m24,m25, test="LR")    #No se pueden hacer porque han aparecido missings en el dataset cuando hemos añadido el factor de enginesize
#AIC(m24,m25)                 
```

Sin embargo, seguiremos avanzando con el nuevo modelo que hemos generado ya que tiene un AIC inferior.

```{r}
residualPlots(m25,id=list(method=cooks.distance(m25),n=10))
marginalModelPlots(m25)
avPlots(m25,id=list(method=hatvalues(m25),n=5))
crPlots(m25,id=list(method=cooks.distance(m25),n=5))
```

En los siguientes plots podemos ver como varía la probabilidad de que el vehículo sea Audi dependiendo de las distintas variables explicativas que aparecen en el modelo.
```{r}
# library(effects)
plot(allEffects(m25))+theme(axis.text.x = element_text(angle=90))
```
Para el caso de mpg, parece que hay una relación de proporcionalidad inversa, de manera que cuanto más consumo tenga el coche, más probable será que este sea Audi.

Para la variable age podemos ver que existe una relación de proporcionalidad directa, de modo que cuanto más viejo sea el coche, más probabilidades tendrá de ser Audi.

En lo que se refiere al factor fuelType, podemos ver como el hecho de que el coche sea diesel o gasolina aumenta las proabilidades de que sea Audi.

Por último, la transmisión manual también aumentaría esta probabilidad, así como los engineSize entre 0 y 9, reduciendose drásticamente la probalidad de que el coche sea Audi para los coches con engineSize entre 9 y 20.


## Interacciones

Procederemos a incorporar las interacciones entre los factores fuelType y transmission. Podemos ver como el AIC del modelo se reduce inmendiatamente. Sin embargo, cuando intentamos usar la función vif para estudiar la co-linealidad de las variables, nos genera un error ya que parece que sí que existe co-linealidad.
```{r}
m26 <- update(m25, ~.*(fuelType+transmission)^2,data=df_train)
#summary(m26) Omitimos debido a la longitud de la salida
#vif(m26)
```
Si realizamos un análisis de la varianza del modelo, vemos como hay algunas interacciones que no son significativas, como pueden ser mpg:fuelType:transmission, age:fuelType:transmission o fuelType:transmission.
```{r}
Anova(m26, test="LR")
```

Usaremos la función step para plantear el mejor modelo posible a partir de una simplificación del que ya tenemos. Si intentamos volver a ejecutar la función vif, vuelve a generar un error que indica que sigue existiendo co-linealidad entre las variables de nuestro modelo, pero en el summary podemos ver como el AIC de este modelo vuelve a ser inferior al del anterior.
```{r}
m27 <- step( m26 )
#vif(m27)
summary(m27)
```

Si realizamos un análisis de la varianza, podemos ver como algunas interacciones parecen no ser significativas.
```{r}
Anova(m27, test="LR")
```

Vamos a proceder a generar un nuevo modelo que excluya las interacciones no rsignificativas del modelo anterior. Nos encontramos que cuando analizamos este modelo, vuelve a aparecer co-linealidad en las variables, de modo que realizaremos una step regression. 
```{r}
m28 <- update(m27, ~.-fuelType:transmission - age:fuelType)
#vif(m28) -> Salta error
```

En este caso, si que podemos ver que ya ha desaparecido la co-linealidad en las variables, y si realizamos el test de anova para los dos modelos, podemos ver que son equivalentes.
```{r}
m29 <- step(m28)
vif(m29)
anova(m29,m28, test="LR")
```

## Diagnóstico
En primer lugar, vamos a ver el summary de nuestro modelo.
```{r}
summary(m29)
```
Como podemos ver, su valor de AIC asociado es de 3639.7. Aunque no es el mejor resultado que hemos conseguido, ya que el m28 tenia 3646.8, este no presenta co-linealidad.

Como hemos comentado anteriormente, con la función vif podemos ver como no aparece co-linealidad entre las variables del modelo.
Si analizamos la varianza del modelo, veremos como todas las variables que aparecen son significativas.
```{r}
vif(m29)
Anova(m29)
```

Vamos a proceder a analizar algunos gráficos del modelo que hemos generado.

Si analizamos los boxplots de las los valores de Hat y las distancias de Cook, podemos ver como hay una serie de elementos que aparecen muy separados, algunos coinciden en los tres gráficos.
```{r}
par(mfrow=c(1,3))
Boxplot(hatvalues(m29),id=c(labels=row.names(df_train)))
Boxplot(cooks.distance(m29),id=c(labels=row.names(df_train)))
Boxplot(abs(rstudent(m29)),id=c(labels=row.names(df_train)))
```

En primer lugar, vamos a observar los valores de los residuos de student. Al haber generado el gráfico con el valor absoluto, solo tendremos que filtrar la parte superior del boxplot. Podemos ver como la cadena de puntos se rompe aproximadamente cerca del 2.3, de manera que vamos a considerar los individuos con residuo de student fuera del intervalo [-2.3, 2.3] como outliers. Haremos lo mismo para las observaciones con distancias de Cook superiores a 0.0035 y Hat values superiores a 0.07.
```{r}
stu_out <- which(abs(rstudent(m29))>2.3);
cook_out <- which(abs(cooks.distance(m29))>0.0035);
hat_out <- which(abs(hatvalues(m29))>0.07);

outs<-unique(c(stu_out,cook_out,hat_out));outs
```

Si echamos un vistazo al plot que nos marca la influencia que tienen las distinas observaciones hacia el modelo, podemos ver como aparecen algunos puntos bastante alejados de las nubes que se crean, algunos con bastante influencia.
```{r}
par(mfrow=c(1,1));
outs2 <- influencePlot(m29, id=c(labels=row.names(df_train)));
outs2 <- labels(outs2)[[1]];
outs2 <- as.numeric(outs2);
outs3 <- unique(c(outs,outs2));outs3
```

Vamos a proceder a crear un nuevo modelo que excluya las observaciones que hemos considerado como outliers.
```{r}
m30 <- update(m29,data=df_train[-outs3,])
summary(m30)
```
Podemos ver como el valor AIC del modelo ha disminuido.

Con la función vif vemos que no existen variables co-lineales en el modelo. Si analizamos la varianza, vemos que todas las variables son significativas.
```{r}
vif(m30)
Anova(m30, test="LR")
```

```{r}
marginalModelPlots(m30)
```

```{r}
m0<-glm(Audi ~ 1, family="binomial", data=df_train[-outs3,])
```

## Bondad del ajuste y capacidad de predicción

Vamos a estudiar la bondad de nuestro modelo y su capacidad de predicción.

En primer lugar, vamos a empezar planteando la distribucion del modelo de forma asimptótica con el test de chi-cuadrado.
```{r}
Anova(m30)
1-pchisq(m30$deviance, m30$df.residual)
```
Como podemos ver, el p-valor de nuestra hipótesis nula es de 0.95, de modo que podemos aceptarla y afirmar que, en efecto, el modelo se ajusta bien a los datos.

Similarmente, si planteamos el estadístico de Pearson X2, nos encontramos que en este caso, aplicando un intervalo de confianza del 95%, deberíamos aceptar nuestra hipótesis nula y afirmar que el modelo se ajusta bien a los datos.
```{r}
X2m30<-sum((resid(m30,"pearson")^2))
1-pchisq( X2m30, m30$df.res)
```

Si aplicamos el test de Pseudo R2, que tiene un rol similar a la suma de los cuadrados de los residuos en una regresion clásica, podemos ver como existen claras discrepancias entre si podemos o no aceptar nuestra hipótesis nula.
```{r}
library(DescTools)
PseudoR2(m30, which='all')
```
Sin embargo, debemos recordar que estos test no funcionan con conjuntos de datos agrupados, como pueden ser los que aparecen en nuestra variable engineSize, o en los factores que hemos incluido en nuestro modelo.

Si planteamos el test de Hoslem, podemos ver como el p-valor de la hipótesis nula es de 0.1336, y podemos aceptar nuestra hipótesis nula, afirmando que el modelo SÍ que se ajusta bien a los datos.
```{r}
library(ResourceSelection)
ll <- which( is.finite(df_test$engineSize_f) )
pred_test <- predict(m30, newdata=df_test[ll,], type="response")
ht <- hoslem.test(as.numeric(df_test$Audi[ll])-1,  pred_test)
ht
```

```{r, include=FALSE, results=FALSE}
library("ROCR")
```

A continuación vamos a general la curva de ROC que nos ayudará a ver de manera gráfica la bondad del ajuste del modelo.

De manera indicativa, un modelo excelente, se acercaría mucho al punto (0,1), mientras que un modelo que se acerca a la recta con y = x sería un modelo malo.
```{r}
pred <- prediction(pred_test, df_test$Audi[ll])
perf <- performance(pred,measure="tpr",x.measure="fpr")

plot(perf,colorize=TRUE,type="l") 
abline(a=0,b=1)

# Área bajo la curva
AUC       <- performance(pred,measure="auc")
AUCaltura <- AUC@y.values

# Punto de corte óptimo
cost.perf <- performance(pred, measure ="cost")
opt.cut   <- pred@cutoffs[[1]][which.min(cost.perf@y.values[[1]])]
#coordenadas del punto de corte óptimo
x<-perf@x.values[[1]][which.min(cost.perf@y.values[[1]])]
y<-perf@y.values[[1]][which.min(cost.perf@y.values[[1]])]
points(x,y, pch=20, col="red")
```
Como podemos ver, nuestro modelo se acerca más a la recta x=y que al punto (0,1), indicando que es bastante mejorable.

Vamos a analizar algunos valores característicos de esta curva.
```{r}
# Área bajo la curva
AUC       <- performance(pred,measure="auc")
AUCaltura <- AUC@y.values

cat("AUC:", AUCaltura[[1]]);
cat("Punto de corte óptimo:",opt.cut)
```
Podemos ver que el área bajo la curva es de 0.687, indicando que es un modelo bastante malo.
Además, el punto de córte óptimo se situa en el (0,0) (No se muy bien como interpretar este resultado, pero muy positivo no debe ser...)

## Matriz de confusión
Vamos a generar la matriz de confusión del modelo que hemos planteado.
```{r}
audi.est <- ifelse(pred_test<0.4,0,1)
tt<-table(audi.est,df_test$Audi[ll]);tt;
```
Si aplicamos las definiciones:
  Sensibilidad: 12 / (12 + 184) = 0.06
  Especificidad: 726 / (28 + 721) = 0.97

Con estos dos conceptos podemos concluir que el modelo responde que NO a casi todo. Vemos como de las 210 observaciones que SÍ que eran Audi, solo ha respondido correctamente al 5%. Por otro lado, vemos como ha acertado casi todos los NO Audi...

Finalmente, si estudiamos la tasa de acierto de nuestro modelo, podemos ver que con el conjunto de validación ha acertado el 77.68% de las veces.
```{r}
100*sum(diag(tt))/sum(tt)
```