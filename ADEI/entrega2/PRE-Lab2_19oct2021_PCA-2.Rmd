---
title: "Load Data and Clean Sample"
author: "Lidia Montero"
date: \today
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  html_document:
    toc: no
    toc_depth: '4'
  word_document:
    toc: no
    toc_depth: '4'
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
subtitle: 'Laboratori 4 - PCA'
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

# Presentation - Títol nivell 1
## R Markdowns document - Títol nivell 2

This is an R Markdown document. 
We are showing some examples of GLMz. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. Use * to provide emphasis such as *italics* and **bold**.

Create lists: Unordered * and +     or   ordered   1. 2.  

  1. Item 1
  2. Item 2
    + Item 2a
    + Item 2b

## Data Description

## Data Description: 100,000 UK Used Car Data set
 
This data dictionary describes data  (https://www.kaggle.com/adityadesai13/used-car-dataset-ford-and-mercedes) - A sample of 5000 trips has been randomly selected from Mercedes, BMW, Volkwagen and Audi manufacturers. So, firstly you have to combine used car from the 4 manufacturers into 1 dataframe.

The cars with engine size 0 are in fact electric cars, nevertheless Mercedes C class, and other given cars are not electric cars,so data imputation is requered. 


  -   manufacturer	Factor: Audi, BMW, Mercedes or Volkswagen
  -   model	Car model
  -   year	registration year
  -   price	price in £
  -   transmission	type of gearbox
  -   mileage	distance used
  -   fuelType	engine fuel
  -   tax	road tax
  -   mpg	Consumption in miles per gallon   
  -   engineSize	size in litres


# Load Required Packages: to be increased over the course

```{r}
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))

requiredPackages <- c("effects","FactoMineR","car","missMDA","mvoutlier","chemometrics", "factoextra","RColorBrewer","ggplot2","dplyr","ggmap","ggthemes","knitr")

#use this function to check if each package is on the local machine
#if a package is installed, it will be loaded
#if any are not, the missing package(s) will be installed and loaded
package.check <- lapply(requiredPackages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})
#verify they are loaded
search()

```

## Load Processed data

```{r}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

setwd("E:/Docencia_UPC/GEI-ADEI/Lab 2")
filepath<-"E:/Docencia_UPC/GEI-ADEI/Lab 1/"
# green_tripdata_2016-01

load(paste0(filepath,"MyOldCars-1000Clean.RData"))
options(contrasts=c("contr.treatment","contr.treatment"))

```

I assume that NA are not present in the variables

# Principal Component Analysis

```{r}
names(df)
vars_con
vars_dis
vars_res

res.pca<-PCA(df[,c(vars_res,vars_con)]) # Does not work: why?
res.pca<-PCA(df[,c("price",vars_con)])  # Not correct: Target should be supplementary
c(vars_res, vars_dis,vars_con, "mout")
summary( df[ , c(vars_res, vars_dis,vars_con, "mout") ] )
res.pca<-PCA(df[,c(vars_res, vars_dis,vars_con)],quali.sup=c(2:13),quanti.sup= c(1)) 

# Multivariant outliers should be included as supplementary observations
ll <- which( df$mout == "YesMOut")
res.pca<-PCA(df[,c(vars_res, vars_dis, vars_con)],quali.sup=c(2:13),quanti.sup= c(1), ind.sup = ll ) 

plot.PCA(res.pca,choix=c("var"),invisible=c("quanti.sup"))
plot.PCA(res.pca,choix=c("var"),invisible=c("var"))
plot.PCA(res.pca,choix=c("var"),invisible=c("quanti.sup","var"))
plot.PCA(res.pca,choix=c("ind"),invisible=c("ind"))
?plot.PCA

### I. Eigenvalues and dominant axes. How many axes we have to interpret? ###
names(res.pca)

summary(res.pca) # Customization is needed
summary(res.pca,nb.dec=2,nbind=1,nbelements=1,ncp=2)

round(res.pca$eig,2)
barplot(res.pca$eig[,1],main="valors propis",names.arg=paste("dim",1:nrow(res.pca$eig)))
sum(res.pca$eig[,1])


### II.  Individuals point of view
### Are they any individuals "too contributive"       ##############
names(res.pca$ind)
round(cbind(res.pca$ind$coord[,1:2],res.pca$ind$cos2[,1:2],res.pca$ind$contrib[,1:2]),2)

# To better understand the axes through the extreme individuals
inds <- res.pca$ind$coord
inds <- as.data.frame(inds)
rang<-inds[order(inds$Dim.1, decreasing = TRUE),]
rang

rang[1,]
res.pca$ind$coord["9708",1]  # Access using row names: invariant
res.pca$ind$coord[row.names(rang)[1:10],1]  # Access using row names: invariant
res.pca$ind$coord[843,1] # Access using row number in current data.frame

df[which(row.names(df)=="44400"),1:18]
df[which(row.names(df) %in% row.names(res.pca$ind$coord[row.names(rang)[1:10],])),1:18]

####
### III. Interpreting the axes:  Variables point of view
### coordinates, quality of representation, contribution of the variables  ##############
### 
round(cbind(res.pca$var$coord[,1:2],res.pca$var$cos2[,1:2],res.pca$var$contrib[,1:2]),2)
round(cbind(res.pca$var$cos2[,1:2],res.pca$var$contrib[,1:2]),2)
# dimdes easies this description from the variables
res.des<-dimdesc(res.pca)
###

res.des$Dim.1$quanti

### we can need more than 2 axes to have a good representation of the clouds
?plot.PCA
plot.PCA(res.pca,choix=c("ind"),cex=0.8)
plot.PCA(res.pca,choix=c("ind"),invisible=c("quali"),axes=c(3,4))
plot.PCA(res.pca,choix=c("var"),axes=c(3,4))

### IV. Perform a PCA taking into account also supplementary variables
### the supplementary variables can be quantitative and/or categorical  
c(vars_res, vars_dis,vars_con)
ll <- which( df$mout == "YesMOut")
res.pca<-PCA(df[,c(vars_res, vars_con)],quali.sup=c(2),quanti.sup= c(1), ind.sup = ll) 
plot(res.pca, choix="ind",invisible=c("ind","ind.sup"), cex=0.7, graph.type = "classic")
# 
lines(res.pca$quali.sup$coord[1:2,1],res.pca$quali.sup$coord[1:2,2],lwd=2,col="black") # Does not work unless graph.type "classic" is set

# Manually producing the plot
plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2],pch=19,col="grey30")
points(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],pch=15,col="magenta")
# lines(res.pca$quali.sup$coord[7:10,1],res.pca$quali.sup$coord[7:10,2],lwd=2,lty=2,col="blue")
text(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],labels=names(res.pca$quali.sup$coord[,1]),col="magenta",cex=0.8)

res.pca$quali.sup$coord


###  Variables point of view
res.des<-dimdesc(res.pca)


###### plots
plot(res.pca, choix="ind", invisible="quali", cex=0.7)
plot(res.pca, choix="ind", invisible="quali",select="contrib 5")
plot(res.pca, choix="var",invisible="quanti.sup",axes=3:4)

```

# Factoextra

```{r}
library(factoextra)
library(ggpubr)
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
fviz_pca_var(res.pca, col.var = "steelblue")

library("corrplot")
corrplot(res.pca$var$cos2, is.corr=FALSE)

fviz_cos2(res.pca, choice = "var", axes = 1:2)

# Color by cos2 values: quality on the factor map
fviz_pca_var(res.pca, col.var = "cos2", gradient.cols = c("white", "blue", "red"), repel = TRUE, legend.title="Nice plot") # Avoid text overlapping

# Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)

```

## HCPC

```{r}
ll <- which( df$mout == "YesMOut")
res.pca<-PCA(df[,c(vars_res, vars_dis, vars_con)],quali.sup=c(2:13),quanti.sup= c(1), ind.sup = ll, ncp = 2) 
res.hcpc<-HCPC(res.pca,order=TRUE) #Computa el clustering jerárquico
# El paquete HPCP ofrece un "corte". Si clickamos en el corte podemos ver un mapa factorial donde los datos aparecen de diferentes colores.

names(res.hcpc)
summary(res.hcpc$data.clust) #nos da el numero de cluster al que asigna cada coche

dim(res.hcpc$data.clust) # Noteu que els outliers multivariants no estan considerats, heu de posar-los en una nou cluster

# res.hcpc<-HCPC(res.pca,min=8,max=12)
### Interpretar los resultados de la clasificación

res.hcpc$desc.axes  # Nos dice la importancia de cada dimensión (del ACP) en la creación de los clusters
  # Ver qué dimension tiene mas importancia no es muy ilustrativo, pero eso con desc.var podemos ver la importancia de las variables originales para cada cluster
res.hcpc$desc.var   # Importancia de las variables originales en cada cluster, como una especie de profiling

res.hcpc$desc.ind   # Esto nos da "el coche típico" de cada cluster. También nos da las distancias entre el coche más alejado de un cluster hacia el centroide de los otros clusters.


### Interpretar los resultados de la clasificación


summary(res.hcpc$data.clust$clust)


### desc.var ###
### A. The description of the clusters by the variables ###
names(res.hcpc$desc.var)

### desc.var$test.chi2 ###
### A.1. The categorical variables which characterizes the clusters ###
res.hcpc$desc.var$test.chi2

### desc.var$category ###
### A.2. The description of each cluster by the categories ##
res.hcpc$desc.var$category

### desc.var$quanti.var ###
### A.3. The quantitative variables which characterizes the clusters ###
res.hcpc$desc.var$quanti.var

### desc.var$quanti ###
### A.4. The description of each cluster by the quantitative variables ###
res.hcpc$desc.var$quanti

### desc.axes ###
### B. The description of the clusters by the axes ###
names(res.hcpc$desc.axes)
res.hcpc$desc.axes$quanti.var
res.hcpc$desc.axes$quanti

### desc.ind ###
### C. The description of the clusters by the individuals ###
names(res.hcpc$desc.ind)
res.hcpc$desc.ind$para
res.hcpc$desc.ind$dist
# Individuals farest from other cluster centers

# Examinar los valores de los individuos que caracterizan a las clases
# La inercia es la suma de distancias ENTRE clases

res.hcpc$call$t$within[1:6]       # Distancia INTRA-clases
res.hcpc$call$t$inert.gain[1:6]   # Nos indica las ganacias de distancias ENTRE clusters de pasar de un número de clusters a otro
sum(res.hcpc$call$t$inert.gain)   # Podemos ver que la suma de las ganancias de distancias entre clusters es igual a la distancia intra-cluster original

# Buscar criterio de elbow -> Número óptimo de clusters

# Otro criterio para número optimo de clusters:
# Dame el número de clusters que aglutinen el ~80% de la variabilidad
#   (res.hcpc$call$t$within[1] - res.hcpc$call$t$within[6]) / res.hcpc$call$t$within[1] -> 0.80
#   (res.hcpc$call$t$within[1] - res.hcpc$call$t$within[4]) / res.hcpc$call$t$within[1] -> 0.70
#   (res.hcpc$call$t$within[1] - res.hcpc$call$t$within[3]) / res.hcpc$call$t$within[1] -> 0.62

######## THE END
####
```

